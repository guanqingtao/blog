## ChatGPT背后的数据库技术体验 - 向量近似搜索之 lance      
                                                                                
### 作者                                                          
digoal                                                          
                                                          
### 日期                                                          
2023-03-30                                                      
                                                
### 标签                                                          
PostgreSQL , PolarDB , lance , 向量相似 , parquet 转换         
                                                          
----                                                          
                                                          
## 背景  
[《ChatGPT背后的数据库技术 - 向量搜索》](../202303/20230330_01.md)  介绍了向量近似搜索的应用场景, 背后的技术. 以及如何在PostgreSQL中使用向量索引, 如何进行向量相似检索.  
  
这篇信息将介绍一下lance这个embedded列存储引擎( [《DuckDB 存储生态: lance(向量存储引擎): Modern columnar data format for ML/超越parquet》](../202303/20230319_01.md) ), 如何将parquet转换为lance, 如何使用lance进行向量近似检索, 如何创建ivfflat索引.   
  
详细参考:  
  
https://eto-ai.github.io/lance/notebooks/quickstart.html  
  
体验环境介绍:  
  
[《在 debian 宿主机中部署和使用 docker (常用docker命令、debian容器常用配置; debian容器部署duckdb和PostgreSQL例子)》](../202303/20230318_01.md)    
  
## 安装必要的包  
```  
apt install -y pip python3  
  
pip3 install pylance duckdb numpy pandas pyarrow   
```  
  
## 如何将parquet转换为lance存储  
  
```  
python3  
  
import lance  
import duckdb  
import numpy as np  
import pandas as pd  
import pyarrow as pa  
import pyarrow.dataset  
import shutil  
  
// 生成parquet文件100万条记录.   
duckdb.sql("COPY (SELECT id,md5(random()::text),now() from range(0,1000000) as t(id)) TO '/home/postgres/tbl.parquet'")   
  
// 将parquet转换为lance  
shutil.rmtree("/home/postgres/tbl.parquet", ignore_errors=True)  
shutil.rmtree("/home/postgres/tbl.parquet.lance", ignore_errors=True)  
  
parquet = pa.dataset.dataset("/home/postgres/tbl.parquet")  
lance.write_dataset(parquet, "/home/postgres/tbl.parquet.lance")  
```  
  
parquet转换后的lance文件如下  
  
```  
root@9b780f5ea2e8:/home/postgres# ll /home/postgres/tbl.parquet  
-rw-r--r-- 1 root root 36M Mar 30 08:48 /home/postgres/tbl.parquet  
  
  
root@9b780f5ea2e8:/home/postgres# ll -R /home/postgres/tbl.parquet.lance  
/home/postgres/tbl.parquet.lance:  
total 20K  
drwxr-xr-x  2 root     root     4.0K Mar 30 08:57 data  
drwxr-xr-x  2 root     root     4.0K Mar 30 08:57 _versions  
-rw-r--r--  1 root     root      220 Mar 30 08:57 _latest.manifest  
drwxr-xr-x  4 root     root     4.0K Mar 30 08:57 .  
drwxr-xr-x 16 postgres postgres 4.0K Mar 30 09:05 ..  
  
/home/postgres/tbl.parquet.lance/data:  
total 54M  
-rw-r--r-- 1 root root  54M Mar 30 08:57 342edd3a-076f-447b-88d3-eb858ae30f13.lance  
drwxr-xr-x 2 root root 4.0K Mar 30 08:57 .  
drwxr-xr-x 4 root root 4.0K Mar 30 08:57 ..  
  
/home/postgres/tbl.parquet.lance/_versions:  
total 12K  
-rw-r--r-- 1 root root  220 Mar 30 08:57 1.manifest  
drwxr-xr-x 4 root root 4.0K Mar 30 08:57 ..  
drwxr-xr-x 2 root root 4.0K Mar 30 08:57 .  
```  
  
## 如何使用lance进行向量近似检索, 如何创建ivfflat索引  
  
1、下载向量数据  
  
```  
curl ftp://ftp.irisa.fr/local/texmex/corpus/sift.tar.gz -o ./sift.tar.gz  
tar -zxvf sift.tar.gz  
  
  
root@9b780f5ea2e8:/home/postgres# cd sift  
root@9b780f5ea2e8:/home/postgres/sift# ll  
total 551M  
-rw-r--r--  1    51993    50100 3.9M Dec 15  2009 sift_groundtruth.ivecs  
-rw-r--r--  1    51993    50100 493M Dec 15  2009 sift_base.fvecs  
-rw-r--r--  1    51993    50100  50M Dec 15  2009 sift_learn.fvecs  
-rw-r--r--  1    51993    50100 5.0M Dec 15  2009 sift_query.fvecs  
drwxr-xr-x  2    51993    50100 4.0K Dec 16  2009 .  
drwxr-xr-x 15 postgres postgres 4.0K Mar 30 09:01 ..  
```  
  
  
2、将下载的向量数据转换为lance 存储  
  
```  
from lance.vector import vec_to_table  
import struct  
  
uri = "/home/postgres/vec_data.lance"  
  
with open("/home/postgres/sift/sift_base.fvecs", mode="rb") as fobj:  
    buf = fobj.read()  
    data = np.array(struct.unpack("<128000000f", buf[4 : 4 + 4 * 1000000 * 128])).reshape((1000000, 128))  
    dd = dict(zip(range(1000000), data))  
  
table = vec_to_table(dd)  
lance.write_dataset(table, uri, max_rows_per_group=8192, max_rows_per_file=1024*1024)  
```  
  
转换后的文件如下  
  
```  
root@9b780f5ea2e8:/home/postgres# ll -R /home/postgres/vec_data.lance   
/home/postgres/vec_data.lance:  
total 20K  
drwxr-xr-x 16 postgres postgres 4.0K Mar 30 09:05 ..  
drwxr-xr-x  2 root     root     4.0K Mar 30 09:05 data  
drwxr-xr-x  2 root     root     4.0K Mar 30 09:05 _versions  
-rw-r--r--  1 root     root      170 Mar 30 09:05 _latest.manifest  
drwxr-xr-x  4 root     root     4.0K Mar 30 09:05 .  
  
/home/postgres/vec_data.lance/data:  
total 496M  
-rw-r--r-- 1 root root 496M Mar 30 09:05 f51a6862-978d-42af-93ba-0ff79a27ab7a.lance  
drwxr-xr-x 2 root root 4.0K Mar 30 09:05 .  
drwxr-xr-x 4 root root 4.0K Mar 30 09:05 ..  
  
/home/postgres/vec_data.lance/_versions:  
total 12K  
-rw-r--r-- 1 root root  170 Mar 30 09:05 1.manifest  
drwxr-xr-x 4 root root 4.0K Mar 30 09:05 ..  
drwxr-xr-x 2 root root 4.0K Mar 30 09:05 .  
```  
  
  
3、使用duckdb查询lance存储的向量数据集  
  
```  
uri = "/home/postgres/vec_data.lance"  
sift1m = lance.dataset(uri)  
  
  
import duckdb  
# if this segfaults make sure duckdb v0.7+ is installed  
samples = duckdb.query("SELECT vector FROM sift1m USING SAMPLE 100").to_df().vector  
samples  
  
  
  
>>> samples  
0     [0.0, 0.0, 54.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  
1     [10.0, 16.0, 31.0, 38.0, 20.0, 38.0, 26.0, 28....  
2     [4.0, 49.0, 1.793662034335766e-43, 10.0, 21.0,...  
3     [0.0, 99.0, 127.0, 70.0, 1.0, 0.0, 0.0, 3.0, 7...  
4     [111.0, 111.0, 13.0, 8.0, 2.0, 0.0, 4.0, 41.0,...  
                            ...                          
95    [124.0, 5.0, 2.0, 1.0, 13.0, 21.0, 9.0, 56.0, ...  
96    [12.0, 55.0, 47.0, 26.0, 21.0, 1.0, 1.0, 1.0, ...  
97    [5.0, 1.0, 5.0, 48.0, 1.793662034335766e-43, 1...  
98    [0.0, 8.0, 2.0, 6.0, 135.0, 132.0, 0.0, 9.0, 3...  
99    [33.0, 95.0, 27.0, 0.0, 0.0, 12.0, 70.0, 91.0,...  
Name: vector, Length: 100, dtype: object  
```  
  
4、搜索与`samples[0]`相似的向量  
  
```  
import time  
  
start = time.time()  
tbl = sift1m.to_table(columns=["id"], nearest={"column": "vector", "q": samples[0], "k": 10})  
end = time.time()  
  
print(f"Time(sec): {end-start}")  
print(tbl.to_pandas())  
  
  
结果如下  
  
>>> print(f"Time(sec): {end-start}")  
Time(sec): 0.7169520854949951  
>>> print(tbl.to_pandas())  
       id                                             vector    score  
0  381618  [0.0, 0.0, 54.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0.0  
1  947928  [6.0, 15.0, 29.0, 30.0, 6.0, 2.0, 0.0, 13.0, 8...  55478.0  
2   25965  [31.0, 10.0, 54.0, 1.0, 7.0, 8.0, 19.0, 97.0, ...  61415.0  
3  253392  [4.0, 0.0, 0.0, 12.0, 8.0, 4.0, 5.0, 40.0, 10....  61468.0  
4  906648  [0.0, 0.0, 37.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0,...  61504.0  
5  766167  [51.0, 22.0, 0.0, 7.0, 72.0, 29.0, 0.0, 0.0, 0...  66262.0  
6  123102  [48.0, 10.0, 25.0, 3.0, 22.0, 1.0, 0.0, 1.0, 1...  66489.0  
7  819702  [21.0, 3.0, 118.0, 4.0, 1.0, 3.0, 4.0, 10.0, 1...  67265.0  
8  876591  [6.0, 0.0, 2.0, 2.0, 8.0, 49.0, 31.0, 0.0, 0.0...  68273.0  
9  570603  [0.0, 7.0, 53.0, 7.0, 6.0, 8.0, 3.0, 45.0, 57....  68409.0  
```  
  
5、创建ivfflat索引  
  
```  
sift1m.create_index("vector",  
                    index_type="IVF_PQ",  
                    num_partitions=256,  # IVF  
                    num_sub_vectors=16)  # PQ  
```  
  
参数选择:   
NOTE If you’re trying this on your own data, make sure your vector `(dimensions / num_sub_vectors) % 8 == 0`, or else index creation will take much longer than expected due to SIMD misalignment  
  
  
6、使用ivfflat索引进行向量近似查询  
  
```  
tot = 0  
for q in samples:  
    start = time.time()  
    tbl = sift1m.to_table(nearest={"column": "vector", "q": q, "k": 10})  
    end = time.time()  
    tot += (end - start)  
  
print(f"Avg(sec): {tot / len(samples)}")  
print(tbl.to_pandas())  
  
  
  
  
>>> print(f"Avg(sec): {tot / len(samples)}")  
Avg(sec): 0.18766138792037965  
>>> print(tbl.to_pandas())  
       id                                             vector     score  
0   43368  [33.0, 95.0, 27.0, 0.0, 0.0, 12.0, 70.0, 91.0,...       0.0  
1   38466  [29.0, 74.0, 52.0, 0.0, 0.0, 0.0, 110.0, 74.0,...   96817.0  
2  135732  [17.0, 92.0, 125.0, 2.0, 0.0, 0.0, 72.0, 82.0,...  100645.0  
3  368706  [74.0, 128.0, 27.0, 0.0, 0.0, 7.0, 23.0, 49.0,...  106299.0  
4  503253  [70.0, 108.0, 5.0, 0.0, 0.0, 0.0, 0.0, 45.0, 9...  109387.0  
5  463005  [28.0, 41.0, 45.0, 0.0, 0.0, 0.0, 62.0, 44.0, ...  111142.0  
6   97935  [41.0, 20.0, 4.0, 0.0, 1.0, 38.0, 125.0, 23.0,...  111424.0  
7  202425  [87.0, 81.0, 87.0, 0.0, 0.0, 0.0, 39.0, 51.0, ...  112321.0  
8  108900  [58.0, 54.0, 18.0, 0.0, 0.0, 0.0, 7.0, 27.0, 3...  112694.0  
9  492546  [40.0, 66.0, 19.0, 1.0, 5.0, 0.0, 28.0, 8.0, 8...  114111.0  
```  
  
  
```  
sift1m.to_table(nearest={"column": "vector",  
                         "q": samples[0],  
                         "k": 10,  
                         "nprobes": 10,  
                         "refine_factor": 5}).to_pandas()  
  
       id                                             vector    score  
0  381618  [0.0, 0.0, 54.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...      0.0  
1  947928  [6.0, 15.0, 29.0, 30.0, 6.0, 2.0, 0.0, 13.0, 8...  55478.0  
2   25965  [31.0, 10.0, 54.0, 1.0, 7.0, 8.0, 19.0, 97.0, ...  61415.0  
3  253392  [4.0, 0.0, 0.0, 12.0, 8.0, 4.0, 5.0, 40.0, 10....  61468.0  
4  906648  [0.0, 0.0, 37.0, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0,...  61504.0  
5  766167  [51.0, 22.0, 0.0, 7.0, 72.0, 29.0, 0.0, 0.0, 0...  66262.0  
6  123102  [48.0, 10.0, 25.0, 3.0, 22.0, 1.0, 0.0, 1.0, 1...  66489.0  
7  819702  [21.0, 3.0, 118.0, 4.0, 1.0, 3.0, 4.0, 10.0, 1...  67265.0  
8  876591  [6.0, 0.0, 2.0, 2.0, 8.0, 49.0, 31.0, 0.0, 0.0...  68273.0  
9  570603  [0.0, 7.0, 53.0, 7.0, 6.0, 8.0, 3.0, 45.0, 57....  68409.0  
```  
  
搜索参数含义:  
  
- q => sample vector // 搜索目标向量  
- k => how many neighbors to return // 最终需要返回多少条近似向量  
- nprobes => how many partitions (in the coarse quantizer) to probe // 搜索多少个离目标向量最近的桶(因为桶也有中心点)  
- refine_factor => controls “re-ranking”. If k=10 and refine_factor=5 then retrieve 50 nearest neighbors by ANN and re-sort using actual distances then return top 10. This improves recall without sacrificing performance too much // 搜索多少倍近似向量条数  
  
NOTE the latencies above include file io as lance currently doesn’t hold anything in memory. Along with index building speed, creating a purely in memory version of the dataset would make the biggest impact on performance.  
  
和PostgreSQL PASE的ivfflat用法非常相似.   
  
  
7、返回向量以及其他字段  
  
```  
tbl = sift1m.to_table()  
tbl = tbl.append_column("item_id", pa.array(range(len(tbl))))  
tbl = tbl.append_column("revenue", pa.array((np.random.randn(len(tbl))+5)*1000))  
tbl.to_pandas()  
  
            id                                             vector  item_id      revenue  
0            0  [0.0, 16.0, 35.0, 5.0, 32.0, 31.0, 14.0, 10.0,...        0  6224.085708  
1            1  [1.8e-43, 14.0, 35.0, 19.0, 20.0, 3.0, 1.0, 13...        1  4548.165025  
2            2  [33.0, 1.8e-43, 0.0, 1.0, 5.0, 3.0, 44.0, 40.0...        2  6431.467233  
3            3  [23.0, 10.0, 1.8e-43, 12.0, 47.0, 14.0, 25.0, ...        3  3941.073563  
4            4  [27.0, 29.0, 21.0, 1.8e-43, 1.0, 1.0, 0.0, 0.0...        4  5147.290599  
...        ...                                                ...      ...          ...  
999995  999995  [8.0, 9.0, 5.0, 0.0, 10.0, 39.0, 72.0, 68.0, 3...   999995  4003.114230  
999996  999996  [3.0, 28.0, 55.0, 29.0, 35.0, 12.0, 1.0, 2.0, ...   999996  5308.172166  
999997  999997  [0.0, 13.0, 41.0, 72.0, 40.0, 9.0, 0.0, 0.0, 0...   999997  4158.079400  
999998  999998  [41.0, 121.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24...   999998  5787.503035  
999999  999999  [2.0, 4.0, 8.0, 8.0, 26.0, 72.0, 63.0, 0.0, 0....   999999  5560.995201  
  
[1000000 rows x 4 columns]  
```  
  
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 云原生分布式开源数据库](https://github.com/ApsaraDB "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、内核开发公开课、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  

## PostgreSQL 大数据场景存储生态: apache arrow - by pg-strom               
                                                                                    
### 作者                                                              
digoal                                                              
                                                              
### 日期                                                              
2023-03-19                                                          
                                                              
### 标签                                                              
PostgreSQL , PolarDB , duckdb , 存储 , parquet , arrow , gpu , cuda , 大数据 , pg-strom                                
                                                              
----                                                              
                                                              
## 背景     
Apache Arrow is a data format of structured data to save in columnar-form and to exchange other applications. Some applications for big-data processing support the format, and it is easy for self-developed applications to use Apache Arrow format since they provides libraries for major programming languages like C,C++ or Python.  
  
Apache Arrow format file internally contains Schema portion to define data structure, and one or more RecordBatch to save columnar-data based on the schema definition. For data types, it supports integers, strint (variable-length), date/time types and so on. Indivisual columnar data has its internal representation according to the data types.  
  
PostgreSQL pg-strom 是计算加速插件, 利用GPU的计算能力. 计算加速通常出现在大数据计算场景, 所以可以说pg-strom把单机版的PG扩展到了大数据场景, 但是光有计算加速还不够, 存储结构也要变化, 才能应对好大数据场景, 也就是接入大数据场景常用的存储生态. arrow就是适合大数据场景的存储引擎之一, 很多大数据计算产品都支持arrow(例如spark, pandas, drill, impala, hbase, kudu, cassandra, parquet等).   
  
pg-strom 贡献的pg2arrow插件可以将PG的数据转成arrow列存储文件. arrow_fdw则可以让PG通过外部表的接口读写arrow格式的列存储文件.   
  
通过arrow的支持, 使得PG融入到了大数据生态中.    
   
对比arrow和parquet:   
- https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/
- https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/
- https://arrow.apache.org/blog/2022/10/17/arrow-parquet-encoding-part-3/
  
## pg2arrow  
https://github.com/heterodb/pg2arrow  
  
```  
$ git clone https://github.com/heterodb/pg-strom.git  
$ cd pg-strom/arrow-tools  
$ make  
$ sudo make install  
$ pg2arrow --help  
Usage:  
  pg2arrow [OPTION] [database] [username]  
  
General options:  
  -d, --dbname=DBNAME   Database name to connect to  
  -c, --command=COMMAND SQL command to run  
  -t, --table=TABLENAME Equivalent to '-c SELECT * FROM TABLENAME'  
      (-c and -t are exclusive, either of them must be given)  
      --inner-join=SUB_COMMAND  
      --outer-join=SUB_COMMAND  
  -o, --output=FILENAME result file in Apache Arrow format  
      --append=FILENAME result Apache Arrow file to be appended  
      (--output and --append are exclusive. If neither of them  
       are given, it creates a temporary file.)  
  -S, --stat[=COLUMNS] embeds min/max statistics for each record batch  
                       COLUMNS is a comma-separated list of the target  
                       columns if partially enabled.  
  
Arrow format options:  
  -s, --segment-size=SIZE size of record batch for each  
  
Connection options:  
  -h, --host=HOSTNAME  database server host  
  -p, --port=PORT      database server port  
  -u, --user=USERNAME  database user name  
  -w, --no-password    never prompt for password  
  -W, --password       force password prompt  
  
Other options:  
      --dump=FILENAME  dump information of arrow file  
      --progress       shows progress of the job  
      --set=NAME:VALUE config option to set before SQL execution  
      --help           shows this message  
  
Report bugs to <pgstrom@heterodb.com>.  
```  
  
https://github.com/heterodb/pg-strom/wiki/803:-Data-exchange-with-SQL-databases-over-Apache-Arrow  
  
`pg2arrow` is a utility command to dump PostgreSQL contents as Apache Arrow files. Below is the simplest example to dump the table table0 in the database postgres to `/path/to/file0.arrow`.  
  
```  
$ pg2arrow -d postgres -c 'SELECT * FROM table0' -o /path/to/file0.arrow  
```  
  
You can supply more complicated query according to the `-c` option. This example tries to cast id to bigint, and fetch 8 charactors from the head of x.  
  
```  
$ pg2arrow -d postgres -c 'SELECT id::bigint,substring(x from 1 for 8) FROM table0' -o /dev/shm/file0.arrow  
```  
  
Not only creation of a new file, you can expand an existing Apache Arrow file using `--append` instead of `-o` option. In this case, the supplied query by `-c` option must be compatible to the schema definitions of the target file.  
  
```  
$ pg2arrow -d postgres -c 'SELECT * FROM table0' --append /path/to/file0.arrow  
```  
   
### demo
1、安装依赖  
  
```  
wget https://packages.ntop.org/apt-stable/bullseye/all/apt-ntop-stable.deb  
apt install ./apt-ntop-stable.deb  
apt update  
apt install -y pfring-drivers-zc-dkms pfring-dkms nprobe ntopng n2disk cento  
```  
  
2、安装pg2arrow  
  
```  
su - postgres  
git clone --depth 1 -b v3.5 https://github.com/heterodb/pg-strom  
cd pg-strom  
cd arrow-tools  
env CC=clang CXX=clang++  make  
  
su - root  
cd /home/postgres/pg-strom/arrow-tools/  
env CC=clang CXX=clang++  make install  
  
....  
mkdir -p /usr/local/bin && \  
install -m 0755 pcap2arrow /usr/local/bin  
mkdir -p /usr/local/bin && \  
install -m 0755 arrow2csv /usr/local/bin  
....  
```  
  
```  
postgres@haier-5000a:~/pg-strom/arrow-tools$ ll  
total 2.8M  
-rw-r--r--  1 postgres postgres 1.3K Mar 18 11:25 sql2arrow.h  
-rw-r--r--  1 postgres postgres  35K Mar 18 11:25 sql2arrow.c  
-rw-r--r--  1 postgres postgres  28K Mar 18 11:25 pgsql_client.c  
-rw-r--r--  1 postgres postgres  90K Mar 18 11:25 pcap2arrow.c  
-rw-r--r--  1 postgres postgres  28K Mar 18 11:25 mysql_client.c  
-rw-r--r--  1 postgres postgres 2.5K Mar 18 11:25 Makefile  
lrwxrwxrwx  1 postgres postgres   15 Mar 18 11:25 float2.h -> ../src/float2.h  
lrwxrwxrwx  1 postgres postgres   20 Mar 18 11:25 arrow_write.c -> ../src/arrow_write.c  
lrwxrwxrwx  1 postgres postgres   20 Mar 18 11:25 arrow_pgsql.c -> ../src/arrow_pgsql.c  
lrwxrwxrwx  1 postgres postgres   20 Mar 18 11:25 arrow_nodes.c -> ../src/arrow_nodes.c  
lrwxrwxrwx  1 postgres postgres   18 Mar 18 11:25 arrow_ipc.h -> ../src/arrow_ipc.h  
lrwxrwxrwx  1 postgres postgres   19 Mar 18 11:25 arrow_defs.h -> ../src/arrow_defs.h  
-rw-r--r--  1 postgres postgres  38K Mar 18 11:25 arrow2csv.c  
drwxr-xr-x 14 postgres postgres 4.0K Mar 18 23:29 ..  
-rw-r--r--  1 postgres postgres 431K Mar 19 14:21 pcap2arrow.o  
-rw-r--r--  1 postgres postgres 185K Mar 19 14:21 arrow_nodes.o  
-rw-r--r--  1 postgres postgres 201K Mar 19 14:21 arrow_write.o  
-rwxr-xr-x  1 postgres postgres 504K Mar 19 14:21 pcap2arrow  
-rw-r--r--  1 postgres postgres 168K Mar 19 14:21 arrow2csv.o  
-rwxr-xr-x  1 postgres postgres 220K Mar 19 14:21 arrow2csv  
-rw-r--r--  1 postgres postgres 103K Mar 19 14:21 __pgsql2arrow.o  
-rw-r--r--  1 postgres postgres 102K Mar 19 14:21 pgsql_client.o  
-rw-r--r--  1 postgres postgres 192K Mar 19 14:21 arrow_pgsql.o  
drwxr-xr-x  2 postgres postgres 4.0K Mar 19 14:21 .  
-rwxr-xr-x  1 postgres postgres 474K Mar 19 14:21 pg2arrow  
```  
  
3、使用帮助  
  
```  
postgres@haier-5000a:~/pg-strom/arrow-tools$ ./pg2arrow --help  
Usage:  
  pg2arrow [OPTION] [database] [username]  
  
General options:  
  -d, --dbname=DBNAME   Database name to connect to  
  -c, --command=COMMAND SQL command to run  
  -t, --table=TABLENAME Equivalent to '-c SELECT * FROM TABLENAME'  
      (-c and -t are exclusive, either of them must be given)  
      --inner-join=SUB_COMMAND  
      --outer-join=SUB_COMMAND  
  -o, --output=FILENAME result file in Apache Arrow format  
      --append=FILENAME result Apache Arrow file to be appended  
      (--output and --append are exclusive. If neither of them  
       are given, it creates a temporary file.)  
  -S, --stat[=COLUMNS] embeds min/max statistics for each record batch  
                       COLUMNS is a comma-separated list of the target  
                       columns if partially enabled.  
  
Arrow format options:  
  -s, --segment-size=SIZE size of record batch for each  
  
Connection options:  
  -h, --host=HOSTNAME  database server host  
  -p, --port=PORT      database server port  
  -u, --user=USERNAME  database user name  
  -w, --no-password    never prompt for password  
  -W, --password       force password prompt  
  
Other options:  
      --dump=FILENAME  dump information of arrow file  
      --progress       shows progress of the job  
      --set=NAME:VALUE config option to set before SQL execution  
      --help           shows this message  
  
Report bugs to <pgstrom@heterodb.com>.  
```  
  
4、将pg数据导出为arrow列存储格式 例子  
  
```  
pg_ctl start  
  
  
postgres@haier-5000a:~/pg-strom/arrow-tools$ ./pg2arrow -h 127.0.0.1 -p 1921 -u postgres -d postgres -c 'select id,md5(random()::text) as info,clock_timestamp() as ts from generate_series(1,10000000) as t(id)' -o /home/postgres/test.arrow -S  
  
postgres@haier-5000a:~/pg-strom/arrow-tools$ ll /home/postgres/test.arrow   
-rw-r--r-- 1 postgres postgres 458M Mar 19 14:29 /home/postgres/test.arrow  
  
  
strings /home/postgres/test.arrow|head -n 15  
ARROW1  
min_values  
max_values  
info  
Asia/Shanghai  
min_values  
max_values  
sql_command  
select id,md5(random()::text) as info,clock_timestamp() as ts from generate_series(1,10000000) as t(id)  
...  
```  
  
  
  
## arrow_fdw  
  
https://heterodb.github.io/pg-strom/arrow_fdw/  
  
```  
CREATE FOREIGN TABLE flogdata (  
    ts        timestamp,  
    sensor_id int,  
    signal1   smallint,  
    signal2   smallint,  
    signal3   smallint,  
    signal4   smallint,  
) SERVER arrow_fdw  
  OPTIONS (file '/path/to/logdata.arrow');  
  
  
IMPORT FOREIGN SCHEMA flogdata  
  FROM SERVER arrow_fdw  
  INTO public  
OPTIONS (file '/path/to/logdata.arrow');  
  
  
=# EXPLAIN VERBOSE  
    SELECT sum(lo_extendedprice*lo_discount) as revenue  
      FROM flineorder,date1  
     WHERE lo_orderdate = d_datekey  
       AND d_year = 1993  
       AND lo_discount between 1 and 3  
       AND lo_quantity < 25;  
                              QUERY PLAN  
--------------------------------------------------------------------------------  
 Aggregate  (cost=12632759.02..12632759.03 rows=1 width=32)  
   Output: sum((pgstrom.psum((flineorder.lo_extendedprice * flineorder.lo_discount))))  
   ->  Custom Scan (GpuPreAgg)  (cost=12632754.43..12632757.49 rows=204 width=8)  
         Output: (pgstrom.psum((flineorder.lo_extendedprice * flineorder.lo_discount)))  
         Reduction: NoGroup  
         GPU Projection: flineorder.lo_extendedprice, flineorder.lo_discount, pgstrom.psum((flineorder.lo_extendedprice * flineorder.lo_discount))  
         Combined GpuJoin: enabled  
         GPU Preference: GPU0 (Tesla V100-PCIE-16GB)  
         ->  Custom Scan (GpuJoin) on public.flineorder  (cost=9952.15..12638126.98 rows=572635 width=12)  
               Output: flineorder.lo_extendedprice, flineorder.lo_discount  
               GPU Projection: flineorder.lo_extendedprice::bigint, flineorder.lo_discount::integer  
               Outer Scan: public.flineorder  (cost=9877.70..12649677.69 rows=4010017 width=16)  
               Outer Scan Filter: ((flineorder.lo_discount >= 1) AND (flineorder.lo_discount <= 3) AND (flineorder.lo_quantity < 25))  
               Depth 1: GpuHashJoin  (nrows 4010017...572635)  
                        HashKeys: flineorder.lo_orderdate  
                        JoinQuals: (flineorder.lo_orderdate = date1.d_datekey)  
                        KDS-Hash (size: 66.06KB)  
               GPU Preference: GPU0 (Tesla V100-PCIE-16GB)  
               NVMe-Strom: enabled  
               referenced: lo_orderdate, lo_quantity, lo_extendedprice, lo_discount  
               files0: /opt/nvme/lineorder_s401.arrow (size: 309.23GB)  
                 lo_orderpriority: 33.61GB  
                 lo_extendedprice: 17.93GB  
                 lo_ordertotalprice: 17.93GB  
                 lo_revenue: 17.93GB  
               ->  Seq Scan on public.date1  (cost=0.00..78.95 rows=365 width=4)  
                     Output: date1.d_datekey  
                     Filter: (date1.d_year = 1993)  
(28 rows)  
```  
  
## parquet fdw
parquet也是大数据场景列存储引擎之一.   
  
https://github.com/adjust/parquet_fdw  
  
- [《DuckDB 存储生态: lance(向量存储引擎): Modern columnar data format for ML/超越parquet》](../202303/20230319_01.md)  
- [《PolarDB 开源版通过 duckdb_fdw 支持 parquet 列存数据文件以及高效OLAP》](../202212/20221209_02.md)  
- [《DuckDB COPY 数据导入导出 - 支持csv, parquet格式, 支持CODEC设置压缩》](../202210/20221026_03.md)  
- [《DuckDB DataLake 场景使用举例 - aliyun OSS对象存储parquet》](../202210/20221026_01.md)  
- [《德说-第140期, duckdb+容器+parquet+对象存储, 实现SaaS场景, 低代码拖拉拽多维度实时分析 降本提效》](../202209/20220913_02.md)  
- [《SQLite3 通过virtual table extension 读取Parquet外部表》](../202209/20220910_03.md)  
- [《DuckDB parquet 分区表 / Delta Lake(数据湖) 应用》](../202209/20220905_01.md)  
- [《DuckDB 采用外部 parquet 格式存储 - tpch 测试 - in_memory VS in_parquet》](../202209/20220901_05.md)  
- [《DuckDB 数据库的数据能不能超出内存限制? 以及推荐的使用方法 - parquet》](../202209/20220901_03.md)  
- [《DuckDB 读写 Parquet 文件 - 同时支持远程s3, oss, http等parquet文件读写》](../202209/20220901_01.md)  
- [《PostgreSQL PGSpider 插件(一款基于FDW的并行的联邦查询插件) - sqlite_fdw, influxdb_fdw, griddb_fdw, mysql_fdw, parquet_s3_fdw》](../202105/20210527_02.md)  
- [《PostgreSQL deltaLake 数据湖用法 - arrow + parquet fdw》](../202005/20200527_04.md)  
  
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 云原生分布式开源数据库](https://github.com/ApsaraDB "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、内核开发公开课、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  

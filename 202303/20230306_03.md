## 使用DuckDB 分解深度嵌套的 JSON，一次一个向量   
                                                                  
### 作者                                            
digoal                                            
                                            
### 日期                                            
2023-03-06                                         
                                  
### 标签                                            
PostgreSQL , PolarDB , DuckDB , json , vector   
                                            
----                                            
                                            
## 背景          
https://duckdb.org/2023/03/03/json.html  
  
DuckDB 试图成为一个易于使用的工具，可以读取各种数据格式。在 0.7.0 版本中，我们添加了对读取 JSON 的支持。JSON 有多种格式和各种模式。结合R, JupySQL 可以很方便的实现数据可视化.   
  
https://duckdb.org/2023/02/24/jupysql.html  
  
https://duckdb.org/docs/api/r  
  
## 正文  
DuckDB 有一个 JSON 扩展，可以通过 SQL 安装和加载：  
  
```  
INSTALL 'json';  
LOAD 'json';  
```  
  
JSON 扩展支持各种函数来创建、读取和操作 JSON 字符串。这些功能类似于PostgreSQL和MySQL等其他数据库提供的 JSON 功能。DuckDB 在内部使用[yyjson](https://github.com/ibireme/yyjson)来解析 JSON，这是一个用 ANSI C 编写的高性能 JSON 库。非常感谢 [yyjson](https://github.com/ibireme/yyjson) 的作者和贡献者！  
  
除了这些功能，DuckDB 现在还可以直接读取 JSON！这是通过自动检测类型和列名，然后将 JSON 中的值转换为 DuckDB 的向量来完成的。自动模式检测极大地简化了对 JSON 数据的处理，并且对 DuckDB 向量的后续查询明显更快！  
  
## 使用 DuckDB 自动读取 JSON  
从0.7.0 版本开始，DuckDB 增加了 JSON 表函数。为了演示这些，我们将阅读`todos.json`一个包含 200 个 [假TODO](https://jsonplaceholder.typicode.com/todos) 项的假 TODO 列表（仅显示前两项）：  
  
```  
cd duckdb/build/release/  
curl -o todos.json https://jsonplaceholder.typicode.com/todos  
```  
  
```  
[  
  {  
    "userId": 1,  
    "id": 1,  
    "title": "delectus aut autem",  
    "completed": false  
  },  
  {  
    "userId": 1,  
    "id": 2,  
    "title": "quis ut nam facilis et officia qui",  
    "completed": false  
  },  
]  
```  
  
每个 TODO 项目都是 JSON 数组中的一个条目，但在 DuckDB 中，我们想要一个表，其中每个条目都是一行。现在（自 2023 年 2 月 DuckDB 的 0.7.0 版本发布以来）就像：  
  
```  
SELECT * FROM 'todos.json';  
```  
  
userId	|id	|title	|completed  
---|---|---|---  
1	|1	|delectus aut autem	|false  
1	|2	|quis ut nam facilis et officia qui	|false  
1	|3	|fugiat veniam minus	|false  
1	|4	|et porro tempora	|true  
1	|5	|laboriosam mollitia et enim quasi adipisci quia provident illum	|false  
  
（注：只显示5行）  
  
现在，找出哪个用户完成了最多的 TODO 项目非常简单：  
  
```  
SELECT userId, sum(completed::int) total_completed  
FROM 'todos.json'  
GROUP BY userId  
ORDER BY total_completed DESC  
LIMIT 1;  
```  
  
userId	|total_completed  
---|---  
5	|12  
  
在内部，DuckDB 识别`.json`中的文件扩展名`'todos.json'`，并`read_json_auto('todos.json')`改为调用。这个功能类似于我们的`read_csv_auto`功能，[自动推断CSV文件的列名和类型](https://duckdb.org/docs/data/csv/auto_detection)。  
  
与我们的其他表函数一样，`read_json_auto`支持通过传递列表来读取多个文件，例如`read_json_auto(['file1.json', 'file2.json'])`，但也支持通配符，例如`read_json_auto('file*.json')`。DuckDB 将并行读取多个文件。  
  
## 换行符分隔的 JSON  
并非所有 JSON 都遵循 中使用的格式`todos.json`，这是一个“记录”数组。换行分隔的 JSON 或 [NDJSON](http://ndjson.org/) 将每一行存储在一个新行上。DuckDB 还支持读取（和写入！）这种格式。首先，让我们将 TODO 列表写成 NDJSON：  
  
```  
COPY (SELECT * FROM 'todos.json') to 'todos2.json';  
```  
  
同样，DuckDB 会识别.json输出文件中的后缀并自动推断出我们要使用(FORMAT JSON). 创建的文件如下所示（仅显示前两条记录）：  
  
```  
{"userId":1,"id":1,"title":"delectus aut autem","completed":false}  
{"userId":1,"id":2,"title":"quis ut nam facilis et officia qui","completed":false}  
```  
  
DuckDB 可以以与原始文件完全相同的方式读取此文件：  
  
```  
SELECT * FROM 'todos2.json';  
```  
  
如果您的 JSON 文件是换行分隔的，DuckDB 可以并行读取。这是用`nd`或`lines`参数指定的：  
  
```  
SELECT * FROM read_ndjson_auto('todos2.json');  
SELECT * FROM read_json_auto('todos2.json', lines='true');  
```  
  
您还可以设置`lines='auto'`为自动检测 JSON 文件是否以换行符分隔。  
  
## 其他 JSON 格式  
如果`read_json`直接使用函数，可以通过参数指定JSON的格式`json_format`。此参数默认为`'auto'`，它告诉 DuckDB 推断我们正在处理的 JSON 类型。第一个`json_format`是`'array_of_records'`，而第二个是`'records'`。可以这样指定：  
  
```  
SELECT * FROM read_json('todos.json', auto_detect=true, json_format='array_of_records');  
SELECT * FROM read_json('todos2.json', auto_detect=true, json_format='records');  
```  
  
其他支持的格式是`'values'` 和 `'array_of_values'`，类似于`'records'` 和 `'array_of_records'`。但是，对于这些格式，每个“记录”都不需要是 JSON 对象，还可以是 JSON 数组、字符串或 JSON 支持的任何内容。  
  
## 手动模式设置JSON结构  
您可能还注意到了参数`auto_detect`。该参数告诉 DuckDB 推断模式，即确定返回列的名称和类型。这些可以像这样手动指定：  
  
```  
SELECT * FROM read_json('todos.json',  
                        columns={userId: 'INT', id: 'INT', title: 'VARCHAR', completed: 'BOOLEAN'},  
                        json_format='array_of_records');  
```  
  
您不必指定所有字段，只需指定您感兴趣的字段即可：  
  
```  
SELECT * FROM read_json('todos.json',  
                        columns={userId: 'INT', completed: 'BOOLEAN'},  
                        json_format='array_of_records');  
```  
  
现在我们知道如何使用新的 DuckDB JSON 表函数，让我们深入分析一下吧！  
  
## GitHub 存档示例  
[GH Archive](https://www.gharchive.org/)是一个记录公共 GitHub 时间线的项目，将其存档，并使其易于访问以供进一步分析。每小时都会上传一个 GZIP 压缩的、以换行符分隔的 JSON 文件，其中包含 GitHub 上的所有公共事件。我已经使用 24 个文件下载了一整天 (2023-02-08) 的活动`wget`并将其存储在名为`gharchive_gz`.  
  
```  
wget https://data.gharchive.org/2023-02-08-0.json.gz  
wget https://data.gharchive.org/2023-02-08-1.json.gz  
...  
wget https://data.gharchive.org/2023-02-08-23.json.gz  
```  
  
请记住，数据是压缩的：  
  
```  
$ du -sh gharchive_gz  
2.3G  gharchive_gz  
$ gunzip -dc gharchive_gz/* | wc -c  
 18396198934  
```  
  
一天的 GitHub 活动总计超过 18GB 的 JSON，使用 GZIP 压缩到 2.3GB。  
  
为了了解数据的外观，我们运行以下查询：  
  
```  
SELECT json_group_structure(json)  
FROM (  
  SELECT *  
  FROM read_ndjson_objects('gharchive_gz/*.json.gz')  
  LIMIT 2048  
);  
```  
  
在这里，我们使用我们的`read_ndjson_objects`函数，该函数将文件中的 JSON 对象读取为原始 JSON，即字符串。该查询从JSON文件`gharchive_gz`目录中读取JSON的前2048条记录并描述结构。  
  
您也可以使用 DuckDB 的扩展名 `[httpfs](https://duckdb.org/docs/extensions/httpfs)` 直接从 GH Archive 查询 JSON 文件:   
  
```  
load 'httpfs';  
  
SELECT json_group_structure(json)  
FROM (  
  SELECT *  
  FROM read_ndjson_objects('https://data.gharchive.org/2023-02-08-0.json.gz')  
  LIMIT 2048  
);  
```  
  
但我们会多次查询这些文件，因此在这种情况下最好下载它们。  
  
我使用[在线 JSON 格式器](https://jsonformatter.curiousconcept.com/)和验证器对结果进行了格式化：  
  
```  
{  
   "id":"VARCHAR",  
   "type":"VARCHAR",  
   "actor":{  
      "id":"UBIGINT",  
      "login":"VARCHAR",  
      "display_login":"VARCHAR",  
      "gravatar_id":"VARCHAR",  
      "url":"VARCHAR",  
      "avatar_url":"VARCHAR"  
   },  
   "repo":{  
      "id":"UBIGINT",  
      "name":"VARCHAR",  
      "url":"VARCHAR"  
   },  
   "payload":{"..."},  
   "public":"BOOLEAN",  
   "created_at":"VARCHAR",  
   "org":{  
      "id":"UBIGINT",  
      "login":"VARCHAR",  
      "gravatar_id":"VARCHAR",  
      "url":"VARCHAR",  
      "avatar_url":"VARCHAR"  
   }  
}  
```  
  
我已经省略`"payload"`了，因为它由深度嵌套的 JSON 组成，并且其格式化结构占用了 1000 多行！  
  
那么，我们到底要处理多少条记录呢？让我们使用 DuckDB 来计算它：  
  
```  
SELECT count(*) count FROM 'gharchive_gz/*.json.gz';  
```  
  
|count|  
|---|  
|4434953|  
  
这大约是每天 440 万个事件，相当于每小时近 20 万个事件。在我的笔记本电脑上，这个查询大约需要 7.3 秒，这是一台配备 M1 芯片和 16GB 内存的 2020 MacBook Pro。这是解压缩 GZIP 压缩和解析每个 JSON 记录所花费的时间。  
  
为了查看在查询中解压缩 GZIP 花费了多少时间，我还创建了一个gharchive包含相同数据但未压缩的目录。对未压缩的数据运行相同的查询大约需要 5.4 秒，几乎快了 2 秒。所以我们变得更快了，但我们也从存储中读取了超过 18GB 的数据，而不是压缩时的 2.3GB。因此，这种比较实际上取决于您的存储速度。我更喜欢压缩数据。  
  
作为旁注，这个查询的速度确实显示了 yyjson 有多快！  
  
那么，GitHub 数据中都有哪些事件呢？  
  
```  
SELECT type, count(*) count  
FROM 'gharchive_gz/*.json.gz'  
GROUP BY type  
ORDER BY count DESC;  
```  
  
type	|count  
---|---  
PushEvent	|2359096  
CreateEvent	|624062  
PullRequestEvent	|366090  
IssueCommentEvent	|238660  
WatchEvent	|231486  
DeleteEvent	|154383  
PullRequestReviewEvent	|131107  
IssuesEvent	|88917  
PullRequestReviewCommentEvent	|79540  
ForkEvent	|64233  
CommitCommentEvent	|36823  
ReleaseEvent	|23004  
MemberEvent	|14872  
PublicEvent	|14500  
GollumEvent	|8180  
  
这个查询大约需要 7.4 秒，比查询时间长不了多少`count(*)`。所以正如我们所看到的，一旦所有内容都被解压和解析，数据分析就会非常快。  
  
最常见的事件类型是`PushEvent`，占所有事件的一半以上，毫不奇怪，这是人们将他们提交的代码推送到 GitHub。最不常见的事件类型是`GollumEvent`，占所有事件的不到 1%，这是维基页面的创建或更新。  
  
如果我们要对同一份数据进行多次分析，每次都解压解析是多余的。相反，我们可以像这样创建一个 DuckDB 表：  
  
```  
CREATE TABLE events AS  
SELECT * EXCLUDE (payload)  
FROM 'gharchive_gz/*.json.gz';  
```  
  
如果您使用的是内存数据库，则大约需要 9 秒。如果您使用的是磁盘上的数据库，这大约需要 13 秒，并导致数据库大小为 444MB。当使用磁盘数据库时，DuckDB 确保表是持久的并执行各种压缩。请注意，我们payload使用方便的子句暂时忽略了该字段EXCLUDE。  
  
为了感受一下我们阅读的内容，我们可以要求 DuckDB 描述该表：  
  
```  
DESCRIBE SELECT * FROM events;  
```  
  
这给了我们以下信息：  
  
cid	|name	|type	|notnull	|dflt_value	|pk  
---|---|---|---|---|---  
0	|id	|BIGINT	|false	| | 	false  
1	|type	|VARCHAR	|false	 | | 	false  
2	|actor	|STRUCT(id UBIGINT, login VARCHAR, display_login VARCHAR, gravatar_id VARCHAR, url VARCHAR, avatar_url VARCHAR)	|false	 | | 	false  
3	|repo	|STRUCT(id UBIGINT, name VARCHAR, url VARCHAR)	|false	| |  	false  
4	|public	|BOOLEAN	|false	 | | 	false  
5	|created_at	|TIMESTAMP	|false	 | | 	false  
6	|org	|STRUCT(id UBIGINT, login VARCHAR, gravatar_id VARCHAR, url VARCHAR, avatar_url VARCHAR)	|false	| |  	false  
  
正如我们所见， JSON 对象的`"actor"`,`"repo"`和`"org"`字段已转换为 DuckDB 结构。该`"id"`列在原始 JSON 中是一个字符串，但已`BIGINT`被 DuckDB 的自动类型检测转换为一个。DuckDB 还可以检测 JSON字符串中的一些不同的`DATE/TIMESTAMP`格式，以及`TIME` 和 `UUID`  
  
现在我们已经创建了表，我们可以像分析任何其他 DuckDB 表一样分析它！让我们看看在这一天duckdb/duckdb GitHub https://github.com/duckdb/duckdb 存储库中有多少活动：  
  
```  
SELECT type, count(*) count  
FROM events  
WHERE repo.name = 'duckdb/duckdb'  
GROUP BY type  
ORDER BY count DESC;  
```  
  
type	|count  
---|---  
PullRequestEvent	|35  
IssueCommentEvent	|30  
WatchEvent	|29  
PushEvent	|15  
PullRequestReviewEvent	|14  
IssuesEvent |	9  
PullRequestReviewCommentEvent	|7  
ForkEvent	|3  
  
这是很多拉取请求活动！请注意，这并不意味着当天打开了 35 个拉取请求，拉取请求中的活动也被计算在内。如果我们搜索那天的拉取请求，我们会发现只有 15 个。这比平时要多，因为大多数 DuckDB 开发人员都在忙于修复 0.7.0 版本的错误。  
  
现在，让我们看看谁最活跃：  
  
```  
SELECT actor.login, count(*) count  
FROM events  
WHERE repo.name = 'duckdb/duckdb'  
  AND type = 'PullRequestEvent'  
GROUP BY actor.login  
ORDER BY count desc  
LIMIT 5;  
```  
  
login	|count  
---|---  
Mytherin	|19  
Mause	|4  
carlopi	|3  
Tmonster	|2  
lnkuiper	|2  
  
不出所料，Mark（Mytherin，DuckDB Labs 联合创始人）最活跃！我的活动（lnkuiper，DuckDB Labs 的软件工程师）也出现了。  
  
## 处理不一致的 JSON 模式  
到目前为止，我们已经忽略了`"payload"`事件的发生。我们忽略了它，因为该字段的内容因事件类型而异。我们可以看到它们与以下查询有何不同：  
  
```  
SELECT json_group_structure(payload) structure  
FROM (SELECT *  
  FROM read_json(  
    'gharchive_gz/*.json.gz',  
    columns={  
      id: 'BIGINT',  
      type: 'VARCHAR',  
      actor: 'STRUCT(id UBIGINT,  
                     login VARCHAR,  
                     display_login VARCHAR,  
                     gravatar_id VARCHAR,  
                     url VARCHAR,  
                     avatar_url VARCHAR)',  
      repo: 'STRUCT(id UBIGINT, name VARCHAR, url VARCHAR)',  
      payload: 'JSON',  
      public: 'BOOLEAN',  
      created_at: 'TIMESTAMP',  
      org: 'STRUCT(id UBIGINT, login VARCHAR, gravatar_id VARCHAR, url VARCHAR, avatar_url VARCHAR)'  
    },  
    lines='true'  
  )  
  WHERE type = 'WatchEvent'  
  LIMIT 2048  
);  
```  
  
|structure|  
|---|  
|`{“action”:”VARCHAR”}`|  
  
对于 `"payload"` 类型的事件，该字段很简单`WatchEvent`。但是，如果我们将类型更改为`PullRequestEvent`，则在使用 JSON 格式化程序进行格式化时，我们会得到超过 500 行的 JSON 结构。我们不想查看所有这些字段，所以我们不能使用我们的自动架构检测，它会尝试获取所有字段。相反，我们可以手动提供我们感兴趣的字段的结构。DuckDB 将跳过读取其他字段。另一种方法是将该`"payload"`字段存储为 DuckDB 的 JSON 数据类型，并在查询时对其进行解析（请参阅本文后面的示例！）。  
  
我已经将`"payload"`事件的JSON 结构剥离PullRequestEvent为我真正感兴趣的东西：  
  
```  
{  
   "action":"VARCHAR",  
   "number":"UBIGINT",  
   "pull_request":{  
      "url":"VARCHAR",  
      "id":"UBIGINT",  
      "title":"VARCHAR",  
      "user":{  
         "login":"VARCHAR",  
         "id":"UBIGINT",  
      },  
      "body":"VARCHAR",  
      "created_at":"TIMESTAMP",  
      "updated_at":"TIMESTAMP",  
      "assignee":{  
         "login":"VARCHAR",  
         "id":"UBIGINT",  
      },  
      "assignees":[  
         {  
            "login":"VARCHAR",  
            "id":"UBIGINT",  
         }  
      ],  
  }  
}  
```  
  
这在技术上不是有效的 JSON，因为有尾随逗号。但是，我们尝试在 DuckDB 中尽可能允许尾随逗号，包括 JSON！  
  
我们现在可以将其插入到`columns`的参数中`read_json`，但我们需要先将其转换为 DuckDB 类型。我很懒，所以我更愿意让 DuckDB 为我做这件事：  
  
```  
SELECT typeof(json_transform('{}', '{  
   "action":"VARCHAR",  
   "number":"UBIGINT",  
   "pull_request":{  
      "url":"VARCHAR",  
      "id":"UBIGINT",  
      "title":"VARCHAR",  
      "user":{  
         "login":"VARCHAR",  
         "id":"UBIGINT",  
      },  
      "body":"VARCHAR",  
      "created_at":"TIMESTAMP",  
      "updated_at":"TIMESTAMP",  
      "assignee":{  
         "login":"VARCHAR",  
         "id":"UBIGINT",  
      },  
      "assignees":[  
         {  
            "login":"VARCHAR",  
            "id":"UBIGINT",  
         }  
      ],  
  }  
}'));  
```  
  
这给了我们一个 DuckDB 类型，我们可以将该类型插入到我们的函数中！请注意，因为我们没有自动检测架构，所以我们必须提供`timestampformat`能够正确解析时间戳的信息。键`"user"`必须用引号括起来，因为它是 SQL 中的保留关键字：  
  
```  
CREATE TABLE pr_events as  
SELECT *  
FROM read_json(  
  'gharchive_gz/*.json.gz',  
  columns={  
    id: 'BIGINT',  
    type: 'VARCHAR',  
    actor: 'STRUCT(id UBIGINT,  
                   login VARCHAR,  
                   display_login VARCHAR,  
                   gravatar_id VARCHAR,  
                   url VARCHAR,  
                   avatar_url VARCHAR)',  
    repo: 'STRUCT(id UBIGINT, name VARCHAR, url VARCHAR)',  
    payload: 'STRUCT(  
                action VARCHAR,  
                number UBIGINT,  
                pull_request STRUCT(  
                  url VARCHAR,  
                  id UBIGINT,  
                  title VARCHAR,  
                  "user" STRUCT(  
                    login VARCHAR,  
                    id UBIGINT  
                  ),  
                  body VARCHAR,  
                  created_at TIMESTAMP,  
                  updated_at TIMESTAMP,  
                  assignee STRUCT(login VARCHAR, id UBIGINT),  
                  assignees STRUCT(login VARCHAR, id UBIGINT)[]  
                )  
              )',  
    public: 'BOOLEAN',  
    created_at: 'TIMESTAMP',  
    org: 'STRUCT(id UBIGINT, login VARCHAR, gravatar_id VARCHAR, url VARCHAR, avatar_url VARCHAR)'  
  },  
  json_format='records',  
  lines='true',  
  timestampformat='%Y-%m-%dT%H:%M:%SZ'  
)  
WHERE type = 'PullRequestEvent';  
```  
  
对于磁盘数据库（结果大小为 478MB），此查询在大约 36 秒内完成，而对于内存数据库，则在 9 秒内完成。如果您不关心保留插入顺序，您可以使用此设置加快查询速度：  
  
```  
SET preserve_insertion_order=false;  
```  
  
使用此设置，查询在磁盘上数据库大约 27 秒内完成，内存数据库在 8.5 秒内完成。磁盘上和内存中情况之间的区别在这里非常大，因为 DuckDB 必须压缩和保存更多数据。  
  
现在我们可以分析拉取请求事件了！让我们看看受让人的最大数量是多少：  
  
```  
SELECT max(length(payload.pull_request.assignees)) max_assignees  
FROM pr_events;  
```  
  
|max_assignees|  
|---|  
|10|  
  
这是很多人审查一个拉取请求！  
  
我们可以检查谁被分配得最多：  
  
```  
WITH assignees AS (  
  SELECT payload.pull_request.assignee.login assignee  
  FROM pr_events  
  UNION ALL  
  SELECT unnest(payload.pull_request.assignees).login assignee  
  FROM pr_events  
)  
SELECT assignee, count(*) count  
FROM assignees  
WHERE assignee NOT NULL  
GROUP BY assignee  
ORDER BY count DESC  
LIMIT 5;  
```  
  
assignee	|count  
---|---  
poad	|494  
vinayakkulkarni	|268  
tmtmtmtm	|198  
fisker	|98  
icemac	|84  
  
这是很多任务！尽管我怀疑这里有重复项。  
  
## 存储为 JSON 以在查询时解析  
指定字段的 JSON 模式`"payload"`很有帮助，因为它允许我们直接分析那里的内容，并且后续查询要快得多。尽管如此，如果模式很复杂，它也会非常麻烦。如果不想指定字段的架构，可以将类型设置为`'JSON'`：  
  
```  
CREATE TABLE pr_events AS  
SELECT *  
FROM read_json(  
  'gharchive_gz/*.json.gz',  
  columns={  
    id: 'BIGINT',  
    type: 'VARCHAR',  
    actor: 'STRUCT(id UBIGINT,  
                   login VARCHAR,  
                   display_login VARCHAR,  
                   gravatar_id VARCHAR,  
                   url VARCHAR,  
                   avatar_url VARCHAR)',  
    repo: 'STRUCT(id UBIGINT, name VARCHAR, url VARCHAR)',  
    payload: 'JSON',  
    public: 'BOOLEAN',  
    created_at: 'TIMESTAMP',  
    org: 'STRUCT(id UBIGINT, login VARCHAR, gravatar_id VARCHAR, url VARCHAR, avatar_url VARCHAR)'  
  },  
  json_format='records',  
  lines='true',  
  timestampformat='%Y-%m-%dT%H:%M:%SZ'  
)  
WHERE type = 'PullRequestEvent';  
```  
  
这会将字段加载`"payload"`为 JSON 字符串，我们可以在查询时使用 DuckDB 的 JSON 函数对其进行分析。例如：  
  
```  
SELECT DISTINCT payload->>'action' AS action, count(*) count  
FROM pr_events  
GROUP BY action  
ORDER BY count DESC;  
```  
  
箭头`->>`是我们函数的简写`json_extract_string`。将整个`"payload"`字段创建为具有类型的列JSON并不是仅获取`"action"`字段的最有效方法，但此示例只是为了展示`read_json`. 查询结果如下表：  
  
action	|count  
---|---  
opened	|189096  
closed	|174914  
reopened	|2080  
  
正如我们所见，只有少数拉取请求被重新打开。  
  
## 结论  
DuckDB 试图成为一个易于使用的工具，可以读取各种数据格式。在 0.7.0 版本中，我们添加了对读取 JSON 的支持。JSON 有多种格式和各种模式。Duckdb 对嵌套类型 ( `LIST`, `STRUCT`) 的丰富支持使其能够将 JSON 完全“切碎”为柱状格式，以进行更高效的分析。  
  
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 云原生分布式开源数据库](https://github.com/ApsaraDB "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、内核开发公开课、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  

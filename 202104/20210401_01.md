## PostgreSQL 14 preview - 增加 结果缓存 exexutor node - GUC 开关 enable_resultcache - 提高join loop性能    
    
### 作者    
digoal    
    
### 日期    
2021-04-01    
    
### 标签    
PostgreSQL , 结果缓存 , executor , enable_resultcache   
    
----    
    
## 背景    
PostgreSQL 14支持结果缓存(GUC enable_resultcache 参数开关), 注意这个结果缓存是指一条SQL内的join节点, 需要多次循环扫描内表时, 如果能用结果缓存可以使用结果缓存, 避免多次扫描提高性能. materialized executor node也有类似效果.   
  
PS: 不是指某条SQL的结果缓存, 下次执行时直接查询结果.   
  
  
  
https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=b6002a796dc0bfe721db5eaa54ba9d24fd9fd416  
  
https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=9eacee2e62d89cab7b004f97c206c4fba4f1d745  
  
```
Add Result Cache executor node (take 2)
author	David Rowley <drowley@postgresql.org>	
Fri, 2 Apr 2021 01:10:56 +0000 (14:10 +1300)
committer	David Rowley <drowley@postgresql.org>	
Fri, 2 Apr 2021 01:10:56 +0000 (14:10 +1300)
commit	9eacee2e62d89cab7b004f97c206c4fba4f1d745
tree	285418c2a4ff3365fe480700dfe58e53a334261f	tree
parent	fe246d1c111d43fd60a1b0afff25ed09b7ae11eb	commit | diff
Add Result Cache executor node (take 2)

Here we add a new executor node type named "Result Cache".  The planner
can include this node type in the plan to have the executor cache the
results from the inner side of parameterized nested loop joins.  This
allows caching of tuples for sets of parameters so that in the event that
the node sees the same parameter values again, it can just return the
cached tuples instead of rescanning the inner side of the join all over
again.  Internally, result cache uses a hash table in order to quickly
find tuples that have been previously cached.

For certain data sets, this can significantly improve the performance of
joins.  The best cases for using this new node type are for join problems
where a large portion of the tuples from the inner side of the join have
no join partner on the outer side of the join.  In such cases, hash join
would have to hash values that are never looked up, thus bloating the hash
table and possibly causing it to multi-batch.  Merge joins would have to
skip over all of the unmatched rows.  If we use a nested loop join with a
result cache, then we only cache tuples that have at least one join
partner on the outer side of the join.  The benefits of using a
parameterized nested loop with a result cache increase when there are
fewer distinct values being looked up and the number of lookups of each
value is large.  Also, hash probes to lookup the cache can be much faster
than the hash probe in a hash join as it's common that the result cache's
hash table is much smaller than the hash join's due to result cache only
caching useful tuples rather than all tuples from the inner side of the
join.  This variation in hash probe performance is more significant when
the hash join's hash table no longer fits into the CPU's L3 cache, but the
result cache's hash table does.  The apparent "random" access of hash
buckets with each hash probe can cause a poor L3 cache hit ratio for large
hash tables.  Smaller hash tables generally perform better.

The hash table used for the cache limits itself to not exceeding work_mem
* hash_mem_multiplier in size.  We maintain a dlist of keys for this cache
and when we're adding new tuples and realize we've exceeded the memory
budget, we evict cache entries starting with the least recently used ones
until we have enough memory to add the new tuples to the cache.

For parameterized nested loop joins, we now consider using one of these
result cache nodes in between the nested loop node and its inner node.  We
determine when this might be useful based on cost, which is primarily
driven off of what the expected cache hit ratio will be.  Estimating the
cache hit ratio relies on having good distinct estimates on the nested
loop's parameters.

For now, the planner will only consider using a result cache for
parameterized nested loop joins.  This works for both normal joins and
also for LATERAL type joins to subqueries.  It is possible to use this new
node for other uses in the future.  For example, to cache results from
correlated subqueries.  However, that's not done here due to some
difficulties obtaining a distinct estimation on the outer plan to
calculate the estimated cache hit ratio.  Currently we plan the inner plan
before planning the outer plan so there is no good way to know if a result
cache would be useful or not since we can't estimate the number of times
the subplan will be called until the outer plan is generated.

The functionality being added here is newly introducing a dependency on
the return value of estimate_num_groups() during the join search.
Previously, during the join search, we only ever needed to perform
selectivity estimations.  With this commit, we need to use
estimate_num_groups() in order to estimate what the hit ratio on the
result cache will be.   In simple terms, if we expect 10 distinct values
and we expect 1000 outer rows, then we'll estimate the hit ratio to be
99%.  Since cache hits are very cheap compared to scanning the underlying
nodes on the inner side of the nested loop join, then this will
significantly reduce the planner's cost for the join.   However, it's
fairly easy to see here that things will go bad when estimate_num_groups()
incorrectly returns a value that's significantly lower than the actual
number of distinct values.  If this happens then that may cause us to make
use of a nested loop join with a result cache instead of some other join
type, such as a merge or hash join.  Our distinct estimations have been
known to be a source of trouble in the past, so the extra reliance on them
here could cause the planner to choose slower plans than it did previous
to having this feature.  Distinct estimations are also fairly hard to
estimate accurately when several tables have been joined already or when a
WHERE clause filters out a set of values that are correlated to the
expressions we're estimating the number of distinct value for.

For now, the costing we perform during query planning for result caches
does put quite a bit of faith in the distinct estimations being accurate.
When these are accurate then we should generally see faster execution
times for plans containing a result cache.  However, in the real world, we
may find that we need to either change the costings to put less trust in
the distinct estimations being accurate or perhaps even disable this
feature by default.  There's always an element of risk when we teach the
query planner to do new tricks that it decides to use that new trick at
the wrong time and causes a regression.  Users may opt to get the old
behavior by turning the feature off using the enable_resultcache GUC.
Currently, this is enabled by default.  It remains to be seen if we'll
maintain that setting for the release.

Additionally, the name "Result Cache" is the best name I could think of
for this new node at the time I started writing the patch.  Nobody seems
to strongly dislike the name. A few people did suggest other names but no
other name seemed to dominate in the brief discussion that there was about
names. Let's allow the beta period to see if the current name pleases
enough people.  If there's some consensus on a better name, then we can
change it before the release.  Please see the 2nd discussion link below
for the discussion on the "Result Cache" name.

Author: David Rowley
Reviewed-by: Andy Fan, Justin Pryzby, Zhihong Yu, Hou Zhijie
Tested-By: Konstantin Knizhnik
Discussion: https://postgr.es/m/CAApHDvrPcQyQdWERGYWx8J%2B2DLUNgXu%2BfOSbQ1UscxrunyXyrQ%40mail.gmail.com
Discussion: https://postgr.es/m/CAApHDvq=yQXr5kqhRviT2RhNKwToaWr9JAN5t+5_PzhuRJ3wvg@mail.gmail.com
```
  
  
```  
Add Result Cache executor node  
author	David Rowley <drowley@postgresql.org>	  
Wed, 31 Mar 2021 23:32:22 +0000 (12:32 +1300)  
committer	David Rowley <drowley@postgresql.org>	  
Wed, 31 Mar 2021 23:32:22 +0000 (12:32 +1300)  
commit	b6002a796dc0bfe721db5eaa54ba9d24fd9fd416  
tree	b724302e02aca3c48b09eede7987e86250991219	tree  
parent	6ec578e60101c3c02533f99715945a0400fb3286	commit | diff  
Add Result Cache executor node  
  
Here we add a new executor node type named "Result Cache".  The planner  
can include this node type in the plan to have the executor cache the  
results from the inner side of parameterized nested loop joins.  This  
allows caching of tuples for sets of parameters so that in the event that  
the node sees the same parameter values again, it can just return the  
cached tuples instead of rescanning the inner side of the join all over  
again.  Internally, result cache uses a hash table in order to quickly  
find tuples that have been previously cached.  
  
For certain data sets, this can significantly improve the performance of  
joins.  The best cases for using this new node type are for join problems  
where a large portion of the tuples from the inner side of the join have  
no join partner on the outer side of the join.  In such cases, hash join  
would have to hash values that are never looked up, thus bloating the hash  
table and possibly causing it to multi-batch.  Merge joins would have to  
skip over all of the unmatched rows.  If we use a nested loop join with a  
result cache, then we only cache tuples that have at least one join  
partner on the outer side of the join.  The benefits of using a  
parameterized nested loop with a result cache increase when there are  
fewer distinct values being looked up and the number of lookups of each  
value is large.  Also, hash probes to lookup the cache can be much faster  
than the hash probe in a hash join as it's common that the result cache's  
hash table is much smaller than the hash join's due to result cache only  
caching useful tuples rather than all tuples from the inner side of the  
join.  This variation in hash probe performance is more significant when  
the hash join's hash table no longer fits into the CPU's L3 cache, but the  
result cache's hash table does.  The apparent "random" access of hash  
buckets with each hash probe can cause a poor L3 cache hit ratio for large  
hash tables.  Smaller hash tables generally perform better.  
  
The hash table used for the cache limits itself to not exceeding work_mem  
* hash_mem_multiplier in size.  We maintain a dlist of keys for this cache  
and when we're adding new tuples and realize we've exceeded the memory  
budget, we evict cache entries starting with the least recently used ones  
until we have enough memory to add the new tuples to the cache.  
  
For parameterized nested loop joins, we now consider using one of these  
result cache nodes in between the nested loop node and its inner node.  We  
determine when this might be useful based on cost, which is primarily  
driven off of what the expected cache hit ratio will be.  Estimating the  
cache hit ratio relies on having good distinct estimates on the nested  
loop's parameters.  
  
For now, the planner will only consider using a result cache for  
parameterized nested loop joins.  This works for both normal joins and  
also for LATERAL type joins to subqueries.  It is possible to use this new  
node for other uses in the future.  For example, to cache results from  
correlated subqueries.  However, that's not done here due to some  
difficulties obtaining a distinct estimation on the outer plan to  
calculate the estimated cache hit ratio.  Currently we plan the inner plan  
before planning the outer plan so there is no good way to know if a result  
cache would be useful or not since we can't estimate the number of times  
the subplan will be called until the outer plan is generated.  
  
The functionality being added here is newly introducing a dependency on  
the return value of estimate_num_groups() during the join search.  
Previously, during the join search, we only ever needed to perform  
selectivity estimations.  With this commit, we need to use  
estimate_num_groups() in order to estimate what the hit ratio on the  
result cache will be.   In simple terms, if we expect 10 distinct values  
and we expect 1000 outer rows, then we'll estimate the hit ratio to be  
99%.  Since cache hits are very cheap compared to scanning the underlying  
nodes on the inner side of the nested loop join, then this will  
significantly reduce the planner's cost for the join.   However, it's  
fairly easy to see here that things will go bad when estimate_num_groups()  
incorrectly returns a value that's significantly lower than the actual  
number of distinct values.  If this happens then that may cause us to make  
use of a nested loop join with a result cache instead of some other join  
type, such as a merge or hash join.  Our distinct estimations have been  
known to be a source of trouble in the past, so the extra reliance on them  
here could cause the planner to choose slower plans than it did previous  
to having this feature.  Distinct estimations are also fairly hard to  
estimate accurately when several tables have been joined already or when a  
WHERE clause filters out a set of values that are correlated to the  
expressions we're estimating the number of distinct value for.  
  
For now, the costing we perform during query planning for result caches  
does put quite a bit of faith in the distinct estimations being accurate.  
When these are accurate then we should generally see faster execution  
times for plans containing a result cache.  However, in the real world, we  
may find that we need to either change the costings to put less trust in  
the distinct estimations being accurate or perhaps even disable this  
feature by default.  There's always an element of risk when we teach the  
query planner to do new tricks that it decides to use that new trick at  
the wrong time and causes a regression.  Users may opt to get the old  
behavior by turning the feature off using the enable_resultcache GUC.  
Currently, this is enabled by default.  It remains to be seen if we'll  
maintain that setting for the release.  
  
Additionally, the name "Result Cache" is the best name I could think of  
for this new node at the time I started writing the patch.  Nobody seems  
to strongly dislike the name. A few people did suggest other names but no  
other name seemed to dominate in the brief discussion that there was about  
names. Let's allow the beta period to see if the current name pleases  
enough people.  If there's some consensus on a better name, then we can  
change it before the release.  Please see the 2nd discussion link below  
for the discussion on the "Result Cache" name.  
  
Author: David Rowley  
Reviewed-by: Andy Fan, Justin Pryzby, Zhihong Yu  
Tested-By: Konstantin Knizhnik  
Discussion: https://postgr.es/m/CAApHDvrPcQyQdWERGYWx8J%2B2DLUNgXu%2BfOSbQ1UscxrunyXyrQ%40mail.gmail.com  
Discussion: https://postgr.es/m/CAApHDvq=yQXr5kqhRviT2RhNKwToaWr9JAN5t+5_PzhuRJ3wvg@mail.gmail.com  
```  
  
```  
   1 -- Perform tests on the Result Cache node.  
   2 -- The cache hits/misses/evictions from the Result Cache node can vary between  
   3 -- machines.  Let's just replace the number with an 'N'.  In order to allow us  
   4 -- to perform validation when the measure was zero, we replace a zero value  
   5 -- with "Zero".  All other numbers are replaced with 'N'.  
   6 create function explain_resultcache(query text, hide_hitmiss bool) returns setof text  
   7 language plpgsql as  
   8 $$  
   9 declare  
  10     ln text;  
  11 begin  
  12     for ln in  
  13         execute format('explain (analyze, costs off, summary off, timing off) %s',  
  14             query)  
  15     loop  
  16         if hide_hitmiss = true then  
  17                 ln := regexp_replace(ln, 'Hits: 0', 'Hits: Zero');  
  18                 ln := regexp_replace(ln, 'Hits: \d+', 'Hits: N');  
  19                 ln := regexp_replace(ln, 'Misses: 0', 'Misses: Zero');  
  20                 ln := regexp_replace(ln, 'Misses: \d+', 'Misses: N');  
  21         end if;  
  22         ln := regexp_replace(ln, 'Evictions: 0', 'Evictions: Zero');  
  23         ln := regexp_replace(ln, 'Evictions: \d+', 'Evictions: N');  
  24         ln := regexp_replace(ln, 'Memory Usage: \d+', 'Memory Usage: N');  
  25         return next ln;  
  26     end loop;  
  27 end;  
  28 $$;  
  29 -- Ensure we get a result cache on the inner side of the nested loop  
  30 SET enable_hashjoin TO off;  
  31 SELECT explain_resultcache('  
  32 SELECT COUNT(*),AVG(t1.unique1) FROM tenk1 t1  
  33 INNER JOIN tenk1 t2 ON t1.unique1 = t2.twenty  
  34 WHERE t2.unique1 < 1000;', false);  
  35                                     explain_resultcache                                       
  36 --------------------------------------------------------------------------------------------  
  37  Aggregate (actual rows=1 loops=1)  
  38    ->  Nested Loop (actual rows=1000 loops=1)  
  39          ->  Bitmap Heap Scan on tenk1 t2 (actual rows=1000 loops=1)  
  40                Recheck Cond: (unique1 < 1000)  
  41                Heap Blocks: exact=333  
  42                ->  Bitmap Index Scan on tenk1_unique1 (actual rows=1000 loops=1)  
  43                      Index Cond: (unique1 < 1000)  
  44          ->  Result Cache (actual rows=1 loops=1000)  
  45                Cache Key: t2.twenty  
  46                Hits: 980  Misses: 20  Evictions: Zero  Overflows: 0  Memory Usage: NkB  
  47                ->  Index Only Scan using tenk1_unique1 on tenk1 t1 (actual rows=1 loops=20)  
  48                      Index Cond: (unique1 = t2.twenty)  
  49                      Heap Fetches: 0  
  50 (13 rows)  
  51   
  52 -- And check we get the expected results.  
  53 SELECT COUNT(*),AVG(t1.unique1) FROM tenk1 t1  
  54 INNER JOIN tenk1 t2 ON t1.unique1 = t2.twenty  
  55 WHERE t2.unique1 < 1000;  
  56  count |        avg           
  57 -------+--------------------  
  58   1000 | 9.5000000000000000  
  59 (1 row)  
  60   
  61 -- Try with LATERAL joins  
  62 SELECT explain_resultcache('  
  63 SELECT COUNT(*),AVG(t2.unique1) FROM tenk1 t1,  
  64 LATERAL (SELECT t2.unique1 FROM tenk1 t2 WHERE t1.twenty = t2.unique1) t2  
  65 WHERE t1.unique1 < 1000;', false);  
  66                                     explain_resultcache                                       
  67 --------------------------------------------------------------------------------------------  
  68  Aggregate (actual rows=1 loops=1)  
  69    ->  Nested Loop (actual rows=1000 loops=1)  
  70          ->  Bitmap Heap Scan on tenk1 t1 (actual rows=1000 loops=1)  
  71                Recheck Cond: (unique1 < 1000)  
  72                Heap Blocks: exact=333  
  73                ->  Bitmap Index Scan on tenk1_unique1 (actual rows=1000 loops=1)  
  74                      Index Cond: (unique1 < 1000)  
  75          ->  Result Cache (actual rows=1 loops=1000)  
  76                Cache Key: t1.twenty  
  77                Hits: 980  Misses: 20  Evictions: Zero  Overflows: 0  Memory Usage: NkB  
  78                ->  Index Only Scan using tenk1_unique1 on tenk1 t2 (actual rows=1 loops=20)  
  79                      Index Cond: (unique1 = t1.twenty)  
  80                      Heap Fetches: 0  
  81 (13 rows)  
  82   
  83 -- And check we get the expected results.  
  84 SELECT COUNT(*),AVG(t2.unique1) FROM tenk1 t1,  
  85 LATERAL (SELECT t2.unique1 FROM tenk1 t2 WHERE t1.twenty = t2.unique1) t2  
  86 WHERE t1.unique1 < 1000;  
  87  count |        avg           
  88 -------+--------------------  
  89   1000 | 9.5000000000000000  
  90 (1 row)  
  91   
  92 -- Reduce work_mem so that we see some cache evictions  
  93 SET work_mem TO '64kB';  
  94 SET enable_mergejoin TO off;  
  95 -- Ensure we get some evictions.  We're unable to validate the hits and misses  
  96 -- here as the number of entries that fit in the cache at once will vary  
  97 -- between different machines.  
  98 SELECT explain_resultcache('  
  99 SELECT COUNT(*),AVG(t1.unique1) FROM tenk1 t1  
 100 INNER JOIN tenk1 t2 ON t1.unique1 = t2.thousand  
 101 WHERE t2.unique1 < 800;', true);  
 102                                      explain_resultcache                                       
 103 ---------------------------------------------------------------------------------------------  
 104  Aggregate (actual rows=1 loops=1)  
 105    ->  Nested Loop (actual rows=800 loops=1)  
 106          ->  Bitmap Heap Scan on tenk1 t2 (actual rows=800 loops=1)  
 107                Recheck Cond: (unique1 < 800)  
 108                Heap Blocks: exact=318  
 109                ->  Bitmap Index Scan on tenk1_unique1 (actual rows=800 loops=1)  
 110                      Index Cond: (unique1 < 800)  
 111          ->  Result Cache (actual rows=1 loops=800)  
 112                Cache Key: t2.thousand  
 113                Hits: Zero  Misses: N  Evictions: N  Overflows: 0  Memory Usage: NkB  
 114                ->  Index Only Scan using tenk1_unique1 on tenk1 t1 (actual rows=1 loops=800)  
 115                      Index Cond: (unique1 = t2.thousand)  
 116                      Heap Fetches: 0  
 117 (13 rows)  
 118   
 119 RESET enable_mergejoin;  
 120 RESET work_mem;  
 121 RESET enable_hashjoin;  
 122 -- Test parallel plans with Result Cache.  
 123 SET min_parallel_table_scan_size TO 0;  
 124 SET parallel_setup_cost TO 0;  
 125 SET parallel_tuple_cost TO 0;  
 126 -- Ensure we get a parallel plan.  
 127 EXPLAIN (COSTS OFF)  
 128 SELECT COUNT(*),AVG(t2.unique1) FROM tenk1 t1,  
 129 LATERAL (SELECT t2.unique1 FROM tenk1 t2 WHERE t1.twenty = t2.unique1) t2  
 130 WHERE t1.unique1 < 1000;  
 131                                   QUERY PLAN                                     
 132 -------------------------------------------------------------------------------  
 133  Finalize Aggregate  
 134    ->  Gather  
 135          Workers Planned: 2  
 136          ->  Partial Aggregate  
 137                ->  Nested Loop  
 138                      ->  Parallel Bitmap Heap Scan on tenk1 t1  
 139                            Recheck Cond: (unique1 < 1000)  
 140                            ->  Bitmap Index Scan on tenk1_unique1  
 141                                  Index Cond: (unique1 < 1000)  
 142                      ->  Result Cache  
 143                            Cache Key: t1.twenty  
 144                            ->  Index Only Scan using tenk1_unique1 on tenk1 t2  
 145                                  Index Cond: (unique1 = t1.twenty)  
 146 (13 rows)  
 147   
 148 -- And ensure the parallel plan gives us the correct results.  
 149 SELECT COUNT(*),AVG(t2.unique1) FROM tenk1 t1,  
 150 LATERAL (SELECT t2.unique1 FROM tenk1 t2 WHERE t1.twenty = t2.unique1) t2  
 151 WHERE t1.unique1 < 1000;  
 152  count |        avg           
 153 -------+--------------------  
 154   1000 | 9.5000000000000000  
 155 (1 row)  
 156   
 157 RESET parallel_tuple_cost;  
 158 RESET parallel_setup_cost;  
 159 RESET min_parallel_table_scan_size;  
```  
    
  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
  
  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
  
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  

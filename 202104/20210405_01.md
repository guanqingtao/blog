## hash join, hash表选择问题: 小表一定是hash table吗?   
    
### 作者    
digoal    
    
### 日期    
2021-04-05     
    
### 标签    
PostgreSQL , cost , 选择性 , 唯一值个数 , 选择性      
    
----    
    
## 背景    
两个表JOIN时, 如果使用hashjoin, 那么hash表的选择和什么有关:  
  
- 唯一值个数  
- 记录数  
- 如果有where filter:   
    - where filter后的记录数  
    - where filter后的唯一值个数  
  
最后, 都是看cost! 代价低的优先.   
  
怎么观察? 用pg_hint_plan可以观察.  
  
[《PostgreSQL hint pg_hint_plan 的详细用法》](../202103/20210327_03.md)    
  
https://pghintplan.osdn.jp/hint_list.html  
  
https://git.postgresql.org/gitweb/?p=postgresql.git;a=blob;f=src/backend/optimizer/path/costsize.c;h=05686d01942b873476b11e9c6014e598ffa2d349;hb=dfc843d465689d2c2af8b0e01c66c51ccaae2343  
  
先rewrite, 然后生成paths, 然后计算cost(计算代价的部分, 可以看到hashjoin 选择不同的hash table时 cost和记录数, mcv(most common values), 唯一值个数都有关系.), 最后选择代价低的.     
  
https://git.postgresql.org/gitweb/?p=postgresql.git;a=blob;f=src/backend/optimizer/README;h=4aefde8bb18d6d61d77a527d73dc1798e8fea2ac;hb=dfc843d465689d2c2af8b0e01c66c51ccaae2343  
  
src/backend/optimizer/path/costsize.c   
  
```  
3714 /*  
3715  * initial_cost_hashjoin  
3716  *    Preliminary estimate of the cost of a hashjoin path.  
3717  *  
3718  * This must quickly produce lower-bound estimates of the path's startup and  
3719  * total costs.  If we are unable to eliminate the proposed path from  
3720  * consideration using the lower bounds, final_cost_hashjoin will be called  
3721  * to obtain the final estimates.  
3722  *  
3723  * The exact division of labor between this function and final_cost_hashjoin  
3724  * is private to them, and represents a tradeoff between speed of the initial  
3725  * estimate and getting a tight lower bound.  We choose to not examine the  
3726  * join quals here (other than by counting the number of hash clauses),  
3727  * so we can't do much with CPU costs.  We do assume that  
3728  * ExecChooseHashTableSize is cheap enough to use here.  
3729  *  
3730  * 'workspace' is to be filled with startup_cost, total_cost, and perhaps  
3731  *      other data to be used by final_cost_hashjoin  
3732  * 'jointype' is the type of join to be performed  
3733  * 'hashclauses' is the list of joinclauses to be used as hash clauses  
3734  * 'outer_path' is the outer input to the join  
3735  * 'inner_path' is the inner input to the join  
3736  * 'extra' contains miscellaneous information about the join  
3737  * 'parallel_hash' indicates that inner_path is partial and that a shared  
3738  *      hash table will be built in parallel  
3739  */  
3740 void  
3741 initial_cost_hashjoin(PlannerInfo *root, JoinCostWorkspace *workspace,  
3742                       JoinType jointype,  
3743                       List *hashclauses,  
3744                       Path *outer_path, Path *inner_path,  
3745                       JoinPathExtraData *extra,  
3746                       bool parallel_hash)  
3747 {  
3748     Cost        startup_cost = 0;  
3749     Cost        run_cost = 0;  
3750     double      outer_path_rows = outer_path->rows;  
3751     double      inner_path_rows = inner_path->rows;  
3752     double      inner_path_rows_total = inner_path_rows;  
3753     int         num_hashclauses = list_length(hashclauses);  
3754     int         numbuckets;  
3755     int         numbatches;  
3756     int         num_skew_mcvs;  
3757     size_t      space_allowed;  /* unused */  
3758   
3759     /* cost of source data */  
3760     startup_cost += outer_path->startup_cost;  
3761     run_cost += outer_path->total_cost - outer_path->startup_cost;  
3762     startup_cost += inner_path->total_cost;  
3763   
3764     /*  
3765      * Cost of computing hash function: must do it once per input tuple. We  
3766      * charge one cpu_operator_cost for each column's hash function.  Also,  
3767      * tack on one cpu_tuple_cost per inner row, to model the costs of  
3768      * inserting the row into the hashtable.  
3769      *  
3770      * XXX when a hashclause is more complex than a single operator, we really  
3771      * should charge the extra eval costs of the left or right side, as  
3772      * appropriate, here.  This seems more work than it's worth at the moment.  
3773      */  
3774     startup_cost += (cpu_operator_cost * num_hashclauses + cpu_tuple_cost)  
3775         * inner_path_rows;  
3776     run_cost += cpu_operator_cost * num_hashclauses * outer_path_rows;  
3777   
3778     /*  
3779      * If this is a parallel hash build, then the value we have for  
3780      * inner_rows_total currently refers only to the rows returned by each  
3781      * participant.  For shared hash table size estimation, we need the total  
3782      * number, so we need to undo the division.  
3783      */  
3784     if (parallel_hash)  
3785         inner_path_rows_total *= get_parallel_divisor(inner_path);  
3786   
3787     /*  
3788      * Get hash table size that executor would use for inner relation.  
3789      *  
3790      * XXX for the moment, always assume that skew optimization will be  
3791      * performed.  As long as SKEW_HASH_MEM_PERCENT is small, it's not worth  
3792      * trying to determine that for sure.  
3793      *  
3794      * XXX at some point it might be interesting to try to account for skew  
3795      * optimization in the cost estimate, but for now, we don't.  
3796      */  
3797     ExecChooseHashTableSize(inner_path_rows_total,  
3798                             inner_path->pathtarget->width,  
3799                             true,   /* useskew */  
3800                             parallel_hash,  /* try_combined_hash_mem */  
3801                             outer_path->parallel_workers,  
3802                             &space_allowed,  
3803                             &numbuckets,  
3804                             &numbatches,  
3805                             &num_skew_mcvs);  
3806   
3807     /*  
3808      * If inner relation is too big then we will need to "batch" the join,  
3809      * which implies writing and reading most of the tuples to disk an extra  
3810      * time.  Charge seq_page_cost per page, since the I/O should be nice and  
3811      * sequential.  Writing the inner rel counts as startup cost, all the rest  
3812      * as run cost.  
3813      */  
3814     if (numbatches > 1)  
3815     {  
3816         double      outerpages = page_size(outer_path_rows,  
3817                                            outer_path->pathtarget->width);  
3818         double      innerpages = page_size(inner_path_rows,  
3819                                            inner_path->pathtarget->width);  
3820   
3821         startup_cost += seq_page_cost * innerpages;  
3822         run_cost += seq_page_cost * (innerpages + 2 * outerpages);  
3823     }  
3824   
3825     /* CPU costs left for later */  
3826   
3827     /* Public result fields */  
3828     workspace->startup_cost = startup_cost;  
3829     workspace->total_cost = startup_cost + run_cost;  
3830     /* Save private data for final_cost_hashjoin */  
3831     workspace->run_cost = run_cost;  
3832     workspace->numbuckets = numbuckets;  
3833     workspace->numbatches = numbatches;  
3834     workspace->inner_rows_total = inner_path_rows_total;  
3835 }  
3836   
3837 /*  
3838  * final_cost_hashjoin  
3839  *    Final estimate of the cost and result size of a hashjoin path.  
3840  *  
3841  * Note: the numbatches estimate is also saved into 'path' for use later  
3842  *  
3843  * 'path' is already filled in except for the rows and cost fields and  
3844  *      num_batches  
3845  * 'workspace' is the result from initial_cost_hashjoin  
3846  * 'extra' contains miscellaneous information about the join  
3847  */  
3848 void  
3849 final_cost_hashjoin(PlannerInfo *root, HashPath *path,  
3850                     JoinCostWorkspace *workspace,  
3851                     JoinPathExtraData *extra)  
3852 {  
3853     Path       *outer_path = path->jpath.outerjoinpath;  
3854     Path       *inner_path = path->jpath.innerjoinpath;  
3855     double      outer_path_rows = outer_path->rows;  
3856     double      inner_path_rows = inner_path->rows;  
3857     double      inner_path_rows_total = workspace->inner_rows_total;  
3858     List       *hashclauses = path->path_hashclauses;  
3859     Cost        startup_cost = workspace->startup_cost;  
3860     Cost        run_cost = workspace->run_cost;  
3861     int         numbuckets = workspace->numbuckets;  
3862     int         numbatches = workspace->numbatches;  
3863     int         hash_mem;  
3864     Cost        cpu_per_tuple;  
3865     QualCost    hash_qual_cost;  
3866     QualCost    qp_qual_cost;  
3867     double      hashjointuples;  
3868     double      virtualbuckets;  
3869     Selectivity innerbucketsize;  
3870     Selectivity innermcvfreq;  
3871     ListCell   *hcl;  
3872   
3873     /* Mark the path with the correct row estimate */  
3874     if (path->jpath.path.param_info)  
3875         path->jpath.path.rows = path->jpath.path.param_info->ppi_rows;  
3876     else  
3877         path->jpath.path.rows = path->jpath.path.parent->rows;  
3878   
3879     /* For partial paths, scale row estimate. */  
3880     if (path->jpath.path.parallel_workers > 0)  
3881     {  
3882         double      parallel_divisor = get_parallel_divisor(&path->jpath.path);  
3883   
3884         path->jpath.path.rows =  
3885             clamp_row_est(path->jpath.path.rows / parallel_divisor);  
3886     }  
3887   
3888     /*  
3889      * We could include disable_cost in the preliminary estimate, but that  
3890      * would amount to optimizing for the case where the join method is  
3891      * disabled, which doesn't seem like the way to bet.  
3892      */  
3893     if (!enable_hashjoin)  
3894         startup_cost += disable_cost;  
3895   
3896     /* mark the path with estimated # of batches */  
3897     path->num_batches = numbatches;  
3898   
3899     /* store the total number of tuples (sum of partial row estimates) */  
3900     path->inner_rows_total = inner_path_rows_total;  
3901   
3902     /* and compute the number of "virtual" buckets in the whole join */  
3903     virtualbuckets = (double) numbuckets * (double) numbatches;  
3904   
3905     /*  
3906      * Determine bucketsize fraction and MCV frequency for the inner relation.  
3907      * We use the smallest bucketsize or MCV frequency estimated for any  
3908      * individual hashclause; this is undoubtedly conservative.  
3909      *  
3910      * BUT: if inner relation has been unique-ified, we can assume it's good  
3911      * for hashing.  This is important both because it's the right answer, and  
3912      * because we avoid contaminating the cache with a value that's wrong for  
3913      * non-unique-ified paths.  
3914      */  
3915     if (IsA(inner_path, UniquePath))  
3916     {  
3917         innerbucketsize = 1.0 / virtualbuckets;  
3918         innermcvfreq = 0.0;  
3919     }  
3920     else  
3921     {  
3922         innerbucketsize = 1.0;  
3923         innermcvfreq = 1.0;  
3924         foreach(hcl, hashclauses)  
3925         {  
3926             RestrictInfo *restrictinfo = lfirst_node(RestrictInfo, hcl);  
3927             Selectivity thisbucketsize;  
3928             Selectivity thismcvfreq;  
3929   
3930             /*  
3931              * First we have to figure out which side of the hashjoin clause  
3932              * is the inner side.  
3933              *  
3934              * Since we tend to visit the same clauses over and over when  
3935              * planning a large query, we cache the bucket stats estimates in  
3936              * the RestrictInfo node to avoid repeated lookups of statistics.  
3937              */  
3938             if (bms_is_subset(restrictinfo->right_relids,  
3939                               inner_path->parent->relids))  
3940             {  
3941                 /* righthand side is inner */  
3942                 thisbucketsize = restrictinfo->right_bucketsize;  
3943                 if (thisbucketsize < 0)  
3944                 {  
3945                     /* not cached yet */  
3946                     estimate_hash_bucket_stats(root,  
3947                                                get_rightop(restrictinfo->clause),  
3948                                                virtualbuckets,  
3949                                                &restrictinfo->right_mcvfreq,  
3950                                                &restrictinfo->right_bucketsize);  
3951                     thisbucketsize = restrictinfo->right_bucketsize;  
3952                 }  
3953                 thismcvfreq = restrictinfo->right_mcvfreq;  
3954             }  
3955             else  
3956             {  
3957                 Assert(bms_is_subset(restrictinfo->left_relids,  
3958                                      inner_path->parent->relids));  
3959                 /* lefthand side is inner */  
3960                 thisbucketsize = restrictinfo->left_bucketsize;  
3961                 if (thisbucketsize < 0)  
3962                 {  
3963                     /* not cached yet */  
3964                     estimate_hash_bucket_stats(root,  
3965                                                get_leftop(restrictinfo->clause),  
3966                                                virtualbuckets,  
3967                                                &restrictinfo->left_mcvfreq,  
3968                                                &restrictinfo->left_bucketsize);  
3969                     thisbucketsize = restrictinfo->left_bucketsize;  
3970                 }  
3971                 thismcvfreq = restrictinfo->left_mcvfreq;  
3972             }  
3973   
3974             if (innerbucketsize > thisbucketsize)  
3975                 innerbucketsize = thisbucketsize;  
3976             if (innermcvfreq > thismcvfreq)  
3977                 innermcvfreq = thismcvfreq;  
3978         }  
3979     }  
3980   
3981     /*  
3982      * If the bucket holding the inner MCV would exceed hash_mem, we don't  
3983      * want to hash unless there is really no other alternative, so apply  
3984      * disable_cost.  (The executor normally copes with excessive memory usage  
3985      * by splitting batches, but obviously it cannot separate equal values  
3986      * that way, so it will be unable to drive the batch size below hash_mem  
3987      * when this is true.)  
3988      */  
3989     hash_mem = get_hash_mem();  
3990     if (relation_byte_size(clamp_row_est(inner_path_rows * innermcvfreq),  
3991                            inner_path->pathtarget->width) >  
3992         (hash_mem * 1024L))  
3993         startup_cost += disable_cost;  
3994   
3995     /*  
3996      * Compute cost of the hashquals and qpquals (other restriction clauses)  
3997      * separately.  
3998      */  
3999     cost_qual_eval(&hash_qual_cost, hashclauses, root);  
4000     cost_qual_eval(&qp_qual_cost, path->jpath.joinrestrictinfo, root);  
4001     qp_qual_cost.startup -= hash_qual_cost.startup;  
4002     qp_qual_cost.per_tuple -= hash_qual_cost.per_tuple;  
4003   
4004     /* CPU costs */  
4005   
4006     if (path->jpath.jointype == JOIN_SEMI ||  
4007         path->jpath.jointype == JOIN_ANTI ||  
4008         extra->inner_unique)  
4009     {  
4010         double      outer_matched_rows;  
4011         Selectivity inner_scan_frac;  
4012   
4013         /*  
4014          * With a SEMI or ANTI join, or if the innerrel is known unique, the  
4015          * executor will stop after the first match.  
4016          *  
4017          * For an outer-rel row that has at least one match, we can expect the  
4018          * bucket scan to stop after a fraction 1/(match_count+1) of the  
4019          * bucket's rows, if the matches are evenly distributed.  Since they  
4020          * probably aren't quite evenly distributed, we apply a fuzz factor of  
4021          * 2.0 to that fraction.  (If we used a larger fuzz factor, we'd have  
4022          * to clamp inner_scan_frac to at most 1.0; but since match_count is  
4023          * at least 1, no such clamp is needed now.)  
4024          */  
4025         outer_matched_rows = rint(outer_path_rows * extra->semifactors.outer_match_frac);  
4026         inner_scan_frac = 2.0 / (extra->semifactors.match_count + 1.0);  
4027   
4028         startup_cost += hash_qual_cost.startup;  
4029         run_cost += hash_qual_cost.per_tuple * outer_matched_rows *  
4030             clamp_row_est(inner_path_rows * innerbucketsize * inner_scan_frac) * 0.5;  
4031   
4032         /*  
4033          * For unmatched outer-rel rows, the picture is quite a lot different.  
4034          * In the first place, there is no reason to assume that these rows  
4035          * preferentially hit heavily-populated buckets; instead assume they  
4036          * are uncorrelated with the inner distribution and so they see an  
4037          * average bucket size of inner_path_rows / virtualbuckets.  In the  
4038          * second place, it seems likely that they will have few if any exact  
4039          * hash-code matches and so very few of the tuples in the bucket will  
4040          * actually require eval of the hash quals.  We don't have any good  
4041          * way to estimate how many will, but for the moment assume that the  
4042          * effective cost per bucket entry is one-tenth what it is for  
4043          * matchable tuples.  
4044          */  
4045         run_cost += hash_qual_cost.per_tuple *  
4046             (outer_path_rows - outer_matched_rows) *  
4047             clamp_row_est(inner_path_rows / virtualbuckets) * 0.05;  
4048   
4049         /* Get # of tuples that will pass the basic join */  
4050         if (path->jpath.jointype == JOIN_ANTI)  
4051             hashjointuples = outer_path_rows - outer_matched_rows;  
4052         else  
4053             hashjointuples = outer_matched_rows;  
4054     }  
4055     else  
4056     {  
4057         /*  
4058          * The number of tuple comparisons needed is the number of outer  
4059          * tuples times the typical number of tuples in a hash bucket, which  
4060          * is the inner relation size times its bucketsize fraction.  At each  
4061          * one, we need to evaluate the hashjoin quals.  But actually,  
4062          * charging the full qual eval cost at each tuple is pessimistic,  
4063          * since we don't evaluate the quals unless the hash values match  
4064          * exactly.  For lack of a better idea, halve the cost estimate to  
4065          * allow for that.  
4066          */  
4067         startup_cost += hash_qual_cost.startup;  
4068         run_cost += hash_qual_cost.per_tuple * outer_path_rows *  
4069             clamp_row_est(inner_path_rows * innerbucketsize) * 0.5;  
4070   
4071         /*  
4072          * Get approx # tuples passing the hashquals.  We use  
4073          * approx_tuple_count here because we need an estimate done with  
4074          * JOIN_INNER semantics.  
4075          */  
4076         hashjointuples = approx_tuple_count(root, &path->jpath, hashclauses);  
4077     }  
4078   
4079     /*  
4080      * For each tuple that gets through the hashjoin proper, we charge  
4081      * cpu_tuple_cost plus the cost of evaluating additional restriction  
4082      * clauses that are to be applied at the join.  (This is pessimistic since  
4083      * not all of the quals may get evaluated at each tuple.)  
4084      */  
4085     startup_cost += qp_qual_cost.startup;  
4086     cpu_per_tuple = cpu_tuple_cost + qp_qual_cost.per_tuple;  
4087     run_cost += cpu_per_tuple * hashjointuples;  
4088   
4089     /* tlist eval costs are paid per output row, not per tuple scanned */  
4090     startup_cost += path->jpath.path.pathtarget->cost.startup;  
4091     run_cost += path->jpath.path.pathtarget->cost.per_tuple * path->jpath.path.rows;  
4092   
4093     path->jpath.path.startup_cost = startup_cost;  
4094     path->jpath.path.total_cost = startup_cost + run_cost;  
4095 }  
```  
  
  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
  
  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
  
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  

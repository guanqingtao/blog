## PostgreSQL 14 preview - Improve performance of Unicode {de,re}composition in the backend    
    
### 作者    
digoal    
    
### 日期    
2020-10-24    
    
### 标签    
PostgreSQL , unicode , 性能      
    
----    
    
## 背景    
unicode 优化, replaces the existing binary search with two perfect hash functions    
for the composition and the decomposition in the backend code, at the    
cost of slightly-larger binaries there (35kB in libpgcommon_srv.a).     
    
40倍性能提升.     
    
https://www.postgresql.org/docs/devel/functions-string.html    
    
https://www.postgresql.org/message-id/CAFBsxsHUuMFCt6-pU+oG-F1==CmEp8wR+O+bRouXWu6i8kXuqA@mail.gmail.com    
    
```    
Having committed the optimization for unicode normalization quick check,    
Michael Paquier suggested I might do the same for decomposition as well. I    
wrote:    
    
> I'll    
> do some performance testing soon. Note that a 25kB increase in size could    
> be present in frontend binaries as well in this case. While looking at    
> decomposition, I noticed that recomposition does a linear search through    
> all 6600+ entries, although it seems only about 800 are valid for that.    
> That could be optimized as well now, since with hashing we have more    
> flexibility in the ordering and can put the recomp-valid entries in front.    
> I'm not yet sure if it's worth the additional complexity. I'll take a look    
> and start a new thread.    
    
The attached patch uses a perfect hash for codepoint decomposition, and for    
recomposing reduces the linear search from 6604 entries to 942.    
    
The performance is very nice, and if I'd known better I would have done    
this first, since the decomp array is as big as the two quick check arrays    
put together:    
    
Normalize, decomp only    
    
select count(normalize(t, NFD)) from (    
select md5(i::text) as t from    
generate_series(1,100000) as i    
) s;    
    
master   patchÏ    
887ms    231ms    
    
select count(normalize(t, NFD)) from (    
select repeat(U&'\00E4\00C5\0958\00F4\1EBF\3300\1FE2\3316\2465\322D', i % 3    
+ 1) as t from    
generate_series(1,100000) as i    
) s;    
    
master   patch    
1110ms   208ms    
    
Normalize, decomp+recomp (note: 100x less data)    
    
select count(normalize(t, NFC)) from (    
select md5(i::text) as t from    
generate_series(1,1000) as i    
) s;    
    
master   patch    
194ms    50.6ms    
    
select count(normalize(t, NFC)) from (    
select repeat(U&'\00E4\00C5\0958\00F4\1EBF\3300\1FE2\3316\2465\322D', i % 3    
+ 1) as t from    
generate_series(1,1000) as i    
) s;    
    
master   patch    
137ms    39.4ms    
    
Quick check is another 2x faster on top of previous gains, since it tests    
canonical class via the decomposition array:    
    
-- all chars are quickcheck YES    
select count(*) from (    
select md5(i::text) as t from    
generate_series(1,100000) as i    
) s    
where t is NFC normalized;    
    
master   patch    
296ms    131ms    
    
Some other considerations:    
- As I alluded above, this adds ~26kB to libpq because of SASLPrep. Since    
the decomp array was reordered to optimize linear search, it can no longer    
be used for binary search. It's possible to build two arrays, one for    
frontend and one for backend, but that's additional complexity. We could    
also force frontend to do a linear search all the time, but that seems    
foolish. I haven't checked if it's possible to exclude the hash from    
backend's libpq.    
- I could split out the two approaches into separate patches, but it'd be    
rather messy.    
    
I'll add a CF entry for this.    
    
--     
John Naylor    
EnterpriseDB: http://www.enterprisedb.com    
The Enterprise PostgreSQL Company    
```    
    
https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=783f0cc64dcc05e3d112a06b1cd181e5a1ca9099    
    
```    
Improve performance of Unicode {de,re}composition in the backend    
author	Michael Paquier <michael@paquier.xyz>	    
Fri, 23 Oct 2020 10:05:46 +0800 (11:05 +0900)    
committer	Michael Paquier <michael@paquier.xyz>	    
Fri, 23 Oct 2020 10:05:46 +0800 (11:05 +0900)    
commit	783f0cc64dcc05e3d112a06b1cd181e5a1ca9099    
tree	ea1b0834526609807e36d5618a8ccb14147bdb1b	tree | snapshot    
parent	7d6d6bce43c60bb7b77237e2cc6ab845646b911f	commit | diff    
Improve performance of Unicode {de,re}composition in the backend    
    
This replaces the existing binary search with two perfect hash functions    
for the composition and the decomposition in the backend code, at the    
cost of slightly-larger binaries there (35kB in libpgcommon_srv.a).  Per    
the measurements done, this improves the speed of the recomposition and    
decomposition by up to 30~40 times for the NFC and NFKC conversions,    
while all other operations get at least 40% faster.  This is not as    
"good" as what libicu has, but it closes the gap a lot as per the    
feedback from Daniel Verite.    
    
The decomposition table remains the same, getting used for the binary    
search in the frontend code, where we care more about the size of the    
libraries like libpq over performance as this gets involved only in code    
paths related to the SCRAM authentication.  In consequence, note that    
the perfect hash function for the recomposition needs to use a new    
inverse lookup array back to to the existing decomposition table.    
    
The size of all frontend deliverables remains unchanged, even with    
--enable-debug, including libpq.    
    
Author: John Naylor    
Reviewed-by: Michael Paquier, Tom Lane    
Discussion: https://postgr.es/m/CAFBsxsHUuMFCt6-pU+oG-F1==CmEp8wR+O+bRouXWu6i8kXuqA@mail.gmail.com    
```    
    
```
normalize ( text [, form ] ) → text    
    
Converts the string to the specified Unicode normalization form. The optional form key word specifies the form: NFC (the default), NFD, NFKC, or NFKD. This function can only be used when the server encoding is UTF8.    
    
normalize(U&'\0061\0308bc', NFC) → U&'\00E4bc'    
```
    
    
  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
  
  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
  
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  

## pg_onnx, 接入开放神经网络集市, load已有模型, 让数据库快速具备推理能力      
                
### 作者                
digoal                
                
### 日期                
2023-09-22                
                
### 标签                
PostgreSQL , PolarDB , 推理 , onnx , ONNX: Open Neural Network Exchange , 开放神经网络集市     
                
----                
                
## 背景          
如果将AI相关的操作按运算量分为2类:       
- 轻计算, 直接在数据库中完成. 例如这篇信息即将提到的pg_onnx, 或者之前提过的 [lantern, lantern_extras](../202309/20230922_01.md)    
- 重计算, 调用大模型集市 API. 例如阿里云灵积.      
    
要让数据库具备AI运算能力, 有2种方法    
- 用本地数据自己训练模型, 训练过程漫长. 例如: [madlib](https://madlib.apache.org/) , [pg4ml](https://gitee.com/seanguo_007/plpgsql_pg4ml)     
- load已经训练好的模型, 例如来自模型集市.  这个就是pg_onnx干的(可将已有模型进行优化, 基于onnx加速推理).  同时还需要可以运行模型的server: ONNX Runtime Server.     
  
已训练好的onnx模型在哪里下载?
- https://github.com/onnx/models     We have standardized on Git LFS (Large File Storage) to store ONNX model files. To download an ONNX model, navigate to the appropriate Github page and click the Download button on the top right.   
    - onnx使用了git lfs来存储大模型原始文件, 所以下载也需要用到lfs客户端: https://git-lfs.com/  
    
    
1、ONNX Runtime is a cross-platform inference and training machine-learning accelerator.    
    
ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator    
    
https://github.com/microsoft/onnxruntime    
    
2、ONNX Runtime Server: The ONNX Runtime Server is a server that provides TCP and HTTP/HTTPS REST APIs for ONNX inference.    
    
https://github.com/kibae/onnxruntime-server     
    
3、pg_onnx: ONNX Runtime integrated with PostgreSQL. Perform ML inference with data in your database.     
    
https://github.com/kibae/pg_onnx     
    
架构图    
    
```mermaid
graph LR
    subgraph P[PostgreSQL]
        direction LR
        PM((postmaster))
        PS1[postgres]
        PS2[postgres]
        PS3[postgres]
        TABLE[(onnx model data)]
        subgraph ONNX[pg_onnx Background Worker]
            direction TB
            OS[onnxruntime-server]
            ONNX1([onnxruntime\nsession:\nmodel1])
            ONNX2([onnxruntime\nsession:\nmodel2])
            OS -. " create/execute session " .-o ONNX1 & ONNX2
        end
        PM -. " fork " .-o PS1 & PS2 & PS3
        PM -. " dynamic background worker " .-o ONNX
        PS3 <-- " import model " --> TABLE
        PS1 & PS2 & PS3 <-- " ONNX operation " --> ONNX
    end
    C[Client trying to use pg_onnx] ==> PS3
```  
        
我尽快集成到以下学习容器中, 方便大家学习.     
  
永久免费的阿里云[云起实验室](https://developer.aliyun.com/adc/scenario/f55dbfac77c0467a9d3cd95ff6697a31)来完成.    
    
如果你本地有docker环境也可以把镜像拉到本地来做实验:    
    
x86_64机器使用以下docker image:    
- [《amd64 image》](../202307/20230710_03.md)    
    
ARM机器使用以下docker image:    
- [《arm64 image》](../202308/20230814_02.md)    
    
## How to use    
    
### Install extension    
    
```sql    
CREATE EXTENSION IF NOT EXISTS pg_onnx;    
```    
    
### Simple Usage    
    
- Import an ONNX file and get the inference results.    
    
```sql    
SELECT pg_onnx_import_model(    
        'sample_model', --------------- model name    
        'v20230101', ------------------ model version     
        PG_READ_BINARY_FILE('/your_model_path/model.onnx')::bytea, -- model binary data    
        '{"cuda": true}'::jsonb, ------ options    
        'sample model' ---------------- description    
    );    
    
SELECT pg_onnx_execute_session(    
        'sample_model', -- model name    
        'v20230101', ----- model version    
        '{    
          "x": [[1], [2], [3]],    
          "y": [[3], [4], [5]],    
          "z": [[5], [6], [7]]    
        }' --------------- inputs    
    );    
```    
    
- Depending on the type and shape of the inputs and outputs of the ML model, you can see different results. Below is an    
  example of the result.    
    
```    
                            pg_onnx_execute                                 
--------------------------------------------------------------------------------    
 {"output": [[0.7488641738891602], [0.8607008457183838], [0.9725375175476074]]}    
```    
    
### Using inference results with triggers    
    
- When data is added, use the BEFORE INSERT trigger to update some columns with ML inference results.    
- *Depending on your ML model, this can have a significant performance impact, so be careful when using it.*    
- [Example](https://github.com/kibae/pg_onnx/blob/main/pg_onnx/expected/05-trigger.out)    
    
```sql    
-- Create a test table    
CREATE TABLE trigger_test    
(    
    id         SERIAL PRIMARY KEY,    
    value1     INT,    
    value2     INT,    
    value3     INT,    
    prediction FLOAT    
);    
    
-- Create a trigger function    
CREATE OR REPLACE FUNCTION trigger_test_insert()    
    RETURNS TRIGGER AS    
$$    
DECLARE    
    result jsonb;    
BEGIN    
    result := pg_onnx_execute_session(    
            'sample_model', 'v20230101',    
            JSONB_BUILD_OBJECT(    
                    'x', ARRAY [[NEW.value1]],    
                    'y', ARRAY [[NEW.value2]],    
                    'z', ARRAY [[NEW.value3]]));    
    
    -- output shape: float[-1,1]    
    -- eg: {"output": [[0.6492120623588562]]}    
    NEW.prediction := result -> 'output' -> 0 -> 0;    
    RETURN NEW;    
END;    
$$    
    LANGUAGE plpgsql;    
    
-- Create a trigger    
CREATE TRIGGER trigger_test_insert    
    BEFORE INSERT    
    ON trigger_test    
    FOR EACH ROW    
EXECUTE PROCEDURE trigger_test_insert();    
```    
    
----    
    
## Functions    
    
- Provides several functions for importing ONNX file and executing and managing it.    
- [ONNX Model Functions](https://github.com/kibae/pg_onnx/wiki/Functions#onnx-model-functions)    
    - [pg_onnx_import_model(TEXT, TEXT, BYTEA, JSONB, TEXT)](https://github.com/kibae/pg_onnx/wiki/Functions#pg_onnx_import_modeltext-text-bytea-jsonb-text)    
    - [pg_onnx_drop_model(TEXT, TEXT)](https://github.com/kibae/pg_onnx/wiki/Functions#pg_onnx_drop_modeltext-text)    
    - [pg_onnx_list_model()](https://github.com/kibae/pg_onnx/wiki/Functions#pg_onnx_list_model)    
    - [pg_onnx_inspect_model_bin(BYTEA)](https://github.com/kibae/pg_onnx/wiki/Functions#pg_onnx_inspect_model_binbytea)    
- [ONNX Session Functions](https://github.com/kibae/pg_onnx/wiki/Functions#onnx-session-functions)    
    - [pg_onnx_create_session(TEXT, TEXT)](https://github.com/kibae/pg_onnx/wiki/Functions#pg_onnx_create_sessiontext-text)    
    - [pg_onnx_describe_session(TEXT, TEXT)](https://github.com/kibae/pg_onnx/wiki/Functions#pg_onnx_describe_sessiontext-text)    
    - [pg_onnx_execute_session(TEXT, TEXT, JSONB)](https://github.com/kibae/pg_onnx/wiki/Functions#pg_onnx_execute_sessiontext-text-jsonb)    
    - [pg_onnx_destroy_session(TEXT, TEXT, JSONB)](https://github.com/kibae/pg_onnx/wiki/Functions#pg_onnx_destroy_sessiontext-text-jsonb)    
    - [pg_onnx_list_session()](https://github.com/kibae/pg_onnx/wiki/Functions#pg_onnx_list_session)    
  
## 安装pg_onnx
使用如下镜像环境进行安装.  
  
x86_64机器使用以下docker image:    
- [《amd64 image》](../202307/20230710_03.md)    
    
ARM机器使用以下docker image:    
- [《arm64 image》](../202308/20230814_02.md)    
  
```
cd /tmp
wget https://github.com/microsoft/onnxruntime/releases/download/v1.15.1/onnxruntime-linux-x64-1.15.1.tgz
mkdir -p /usr/local/onnxruntime
tar vzxf onnxruntime-linux-x64-1.15.1.tgz -C /usr/local/onnxruntime --strip-components=1
echo "/usr/local/onnxruntime/lib" > /etc/ld.so.conf.d/onnxruntime.conf
ldconfig

cd /tmp 
git clone --depth 1 -b 35-v121-compile-error https://github.com/kibae/onnxruntime-server
cd onnxruntime-server
cmake -B build -S . -DCMAKE_BUILD_TYPE=Release
cmake --build build --parallel 4
cmake --install build --prefix /usr/local/onnxruntime-server

cd /tmp
git clone --depth 1 -b v1.15.1 https://github.com/microsoft/onnxruntime
cp /tmp/onnxruntime/include/onnxruntime/core/session/* /usr/local/onnxruntime/

cd /tmp
git clone --depth 1 --recursive https://github.com/kibae/pg_onnx.git
cd pg_onnx
cmake -B build -S . -DCMAKE_BUILD_TYPE=Release
cmake --build build --target pg_onnx --parallel 4 
cmake --install build/pg_onnx
```
   
编译完可以把文件拷贝到容器外面下次直接使用:   
```
docker cp pg:/usr/share/postgresql/14/extension/pg_onnx--1.2.1.sql ~/pg14/  
docker cp pg:/usr/share/postgresql/14/extension/pg_onnx.control ~/pg14/  
docker cp pg:/usr/lib/postgresql/14/lib/pg_onnx.so ~/pg14/  
```
  
如果使用以上包拷贝方式, 还需要onnxruntime的lib包也上传到容器镜像中.  
```
ldd 查看到so依赖如下:
root@51feb074d0a3:~# ldd /usr/lib/postgresql/14/lib/pg_onnx.so
	linux-vdso.so.1 (0x00007ffef778f000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f0d6cdaa000)
	libonnxruntime.so.1.15.1 => /usr/local/onnxruntime/lib/libonnxruntime.so.1.15.1 (0x00007f0d6bdb4000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f0d6bbe7000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f0d6bbcd000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f0d6b9f9000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f0d6cea3000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f0d6b9f3000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f0d6b9e7000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f0d6b8a3000)
root@51feb074d0a3:~# ldd /usr/local/onnxruntime/lib/libonnxruntime.so.1.15.1
	linux-vdso.so.1 (0x00007ffeb77f3000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f77a910e000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f77a9104000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f77a90e2000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f77a8f15000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f77a8dd1000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f77a8db7000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f77a8be1000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f77aa11d000)
```
   
## 下载已有模型
在onnx models开源项目中收集整理了大量模型:   
- https://github.com/onnx/models   
  
onnx runtime手册
- https://onnxruntime.ai/  
  
模型文件非常大, 需要使用git lfs来下载:  
```
cd /tmp
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash
apt-get install -y git-lfs
git lfs install
```
  
查询模型文件, 使用lfs下载:  
```
# 列出onnx model项目中收集的模型, 这些大模型文件存储在lfs:
cd /tmp/models
git lfs ls-files

# 查看lfs文件概要
cat text/machine_comprehension/gpt-2/model/gpt2-lm-head-10.onnx

version https://git-lfs.github.com/spec/v1
oid sha256:12fbb1ec2d2d70c8ebd21a290a348a4109447b98582af64c6f93b6526d5c8f35
size 664871060

# 下载存储在lfs的指定模型文件
git lfs fetch -I text/machine_comprehension/gpt-2/model/gpt2-lm-head-10.onnx -X ""
```
    
## 参考    
https://github.com/microsoft/onnxruntime    
    
https://github.com/kibae/onnxruntime-server    
    
https://github.com/kibae/pg_onnx    
    
https://onnx.ai/onnx/intro/    
    
https://onnx.ai/onnx/intro/concepts.html#functions    
  
https://github.com/onnx/models  
  
https://onnxruntime.ai/  
  
模型排行榜:
- https://opencompass.org.cn/leaderboard-llm
    
    
  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB 云原生分布式开源数据库](https://github.com/ApsaraDB "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、内核开发公开课、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  

## DuckDB 数据库的数据能不能超出内存限制? 以及推荐的使用方法 - parquet      
    
### 作者    
digoal    
    
### 日期    
2022-09-01    
    
### 标签    
PostgreSQL , DuckDB , parquet , 内存限制   
    
----    
    
## 背景    
DuckDB是内存数据库, 那么数据能不能超出内存?    
  
DuckDB可以纯内存启动, 也可以以数据文件启动.    
  
当以数据文件启动时, 数据能超出内存限制.   
  
即使是纯内存启动, 由于DuckDB支持读写外部数据文件(csv, parquet等格式), 所以数据也能超出内存.   
  
推荐用法:    
- 数据都尽量存储在parquet中, 内存干嘛用呢? 计算时, hash table, 排序等用到内存.  这样duckdb能管理的数据就可以无限大.    
- duckdb自己的数据文件会膨胀, 由于数据都存储在parquet中, 膨胀后建议导出schema定义, 重新启动一份新的数据文件然后导入schema, 老的数据文件删掉即可.    
- 由于parquet支持pushdown filter\projection, 支持分区, 支持并行.  查询速度是非常快的.   甚至可以使用远程parquet文件存储 (s3, https, oss) .   
  
例子:  
  
```  
IT-C02YW2EFLVDL:release digoal$ ./duckdb ./digoal.db  
v0.4.1-dev2371 3825e0ee7  
Enter ".help" for usage hints.  
```  
  
  
查询内存限制设置: `max_memory, memory_limit`  
  
  
```  
D SELECT * FROM duckdb_settings();  
┌────────────────────────────────┬─────────────────┬────────────────────────────────────────────────────────────────────────────────────┬────────────┐  
│              name              │      value      │                                    description                                     │ input_type │  
├────────────────────────────────┼─────────────────┼────────────────────────────────────────────────────────────────────────────────────┼────────────┤  
│ access_mode                    │ automatic       │ Access mode of the database (AUTOMATIC, READ_ONLY or READ_WRITE)                   │ VARCHAR    │  
│ checkpoint_threshold           │ 16.7MB          │ The WAL size threshold at which to automatically trigger a checkpoint (e.g. 1GB... │ VARCHAR    │  
│ debug_checkpoint_abort         │ NULL            │ DEBUG SETTING: trigger an abort while checkpointing for testing purposes           │ VARCHAR    │  
│ debug_force_external           │ False           │ DEBUG SETTING: force out-of-core computation for operators that support it, use... │ BOOLEAN    │  
│ debug_force_no_cross_product   │ False           │ DEBUG SETTING: Force disable cross product generation when hyper graph isn't co... │ BOOLEAN    │  
│ debug_many_free_list_blocks    │ False           │ DEBUG SETTING: add additional blocks to the free list                              │ BOOLEAN    │  
│ debug_window_mode              │ NULL            │ DEBUG SETTING: switch window mode to use                                           │ VARCHAR    │  
│ default_collation              │                 │ The collation setting used when none is specified                                  │ VARCHAR    │  
│ default_order                  │ asc             │ The order type used when none is specified (ASC or DESC)                           │ VARCHAR    │  
│ default_null_order             │ nulls_first     │ Null ordering used when none is specified (NULLS_FIRST or NULLS_LAST)              │ VARCHAR    │  
│ disabled_optimizers            │                 │ DEBUG SETTING: disable a specific set of optimizers (comma separated)              │ VARCHAR    │  
│ enable_external_access         │ True            │ Allow the database to access external state (through e.g. loading/installing mo... │ BOOLEAN    │  
│ allow_unsigned_extensions      │ False           │ Allow to load extensions with invalid or missing signatures                        │ BOOLEAN    │  
│ enable_object_cache            │ False           │ Whether or not object cache is used to cache e.g. Parquet metadata                 │ BOOLEAN    │  
│ enable_profiling               │ NULL            │ Enables profiling, and sets the output format (JSON, QUERY_TREE, QUERY_TREE_OPT... │ VARCHAR    │  
│ enable_progress_bar            │ False           │ Enables the progress bar, printing progress to the terminal for long queries       │ BOOLEAN    │  
│ explain_output                 │ physical_only   │ Output of EXPLAIN statements (ALL, OPTIMIZED_ONLY, PHYSICAL_ONLY)                  │ VARCHAR    │  
│ external_threads               │ 0               │ The number of external threads that work on DuckDB tasks.                          │ BIGINT     │  
│ file_search_path               │                 │ A comma separated list of directories to search for input files                    │ VARCHAR    │  
│ force_compression              │ NULL            │ DEBUG SETTING: forces a specific compression method to be used                     │ VARCHAR    │  
│ home_directory                 │                 │ Sets the home directory used by the system                                         │ VARCHAR    │  
│ log_query_path                 │ NULL            │ Specifies the path to which queries should be logged (default: empty string, qu... │ VARCHAR    │  
│ max_expression_depth           │ 1000            │ The maximum expression depth limit in the parser. WARNING: increasing this sett... │ UBIGINT    │  
│ max_memory                     │ 13.7GB          │ The maximum memory of the system (e.g. 1GB)                                        │ VARCHAR    │  
│ memory_limit                   │ 13.7GB          │ The maximum memory of the system (e.g. 1GB)                                        │ VARCHAR    │  
│ null_order                     │ nulls_first     │ Null ordering used when none is specified (NULLS_FIRST or NULLS_LAST)              │ VARCHAR    │  
│ perfect_ht_threshold           │ 12              │ Threshold in bytes for when to use a perfect hash table (default: 12)              │ BIGINT     │  
│ preserve_identifier_case       │ True            │ Whether or not to preserve the identifier case, instead of always lowercasing a... │ BOOLEAN    │  
│ preserve_insertion_order       │ True            │ Whether or not to preserve insertion order. If set to false the system is allow... │ BOOLEAN    │  
│ profiler_history_size          │ NULL            │ Sets the profiler history size                                                     │ BIGINT     │  
│ profile_output                 │                 │ The file to which profile output should be saved, or empty to print to the term... │ VARCHAR    │  
│ profiling_mode                 │ NULL            │ The profiling mode (STANDARD or DETAILED)                                          │ VARCHAR    │  
│ profiling_output               │                 │ The file to which profile output should be saved, or empty to print to the term... │ VARCHAR    │  
│ progress_bar_time              │ 2000            │ Sets the time (in milliseconds) how long a query needs to take before we start ... │ BIGINT     │  
│ schema                         │                 │ Sets the default search schema. Equivalent to setting search_path to a single v... │ VARCHAR    │  
│ search_path                    │                 │ Sets the default search search path as a comma-separated list of values            │ VARCHAR    │  
│ temp_directory                 │ ./digoal.db.tmp │ Set the directory to which to write temp files                                     │ VARCHAR    │  
│ threads                        │ 8               │ The number of total threads used by the system.                                    │ BIGINT     │  
│ wal_autocheckpoint             │ 16.7MB          │ The WAL size threshold at which to automatically trigger a checkpoint (e.g. 1GB... │ VARCHAR    │  
│ worker_threads                 │ 8               │ The number of total threads used by the system.                                    │ BIGINT     │  
│ Calendar                       │ gregorian       │ The current calendar                                                               │ VARCHAR    │  
│ TimeZone                       │ Asia/Shanghai   │ The current time zone                                                              │ VARCHAR    │  
│ s3_uploader_thread_limit       │                 │ S3 Uploader global thread limit (default 50)                                       │ UBIGINT    │  
│ s3_session_token               │                 │ S3 Session Token                                                                   │ VARCHAR    │  
│ s3_secret_access_key           │                 │ S3 Access Key                                                                      │ VARCHAR    │  
│ s3_url_style                   │                 │ S3 url style ('vhost' (default) or 'path')                                         │ VARCHAR    │  
│ s3_uploader_max_filesize       │                 │ S3 Uploader max filesize (between 50GB and 5TB, default 800GB)                     │ VARCHAR    │  
│ s3_uploader_max_parts_per_file │                 │ S3 Uploader max parts per file (between 1 and 10000, default 10000)                │ UBIGINT    │  
│ s3_endpoint                    │                 │ S3 Endpoint (default 's3.amazonaws.com')                                           │ VARCHAR    │  
│ s3_use_ssl                     │                 │ S3 use SSL (default true)                                                          │ BOOLEAN    │  
│ httpfs_timeout                 │                 │ HTTP timeout read/write/connection/retry (default 30000ms)                         │ UBIGINT    │  
│ s3_region                      │                 │ S3 Region                                                                          │ VARCHAR    │  
│ binary_as_string               │                 │ In Parquet files, interpret binary data as a string.                               │ BOOLEAN    │  
│ s3_access_key_id               │                 │ S3 Access Key ID                                                                   │ VARCHAR    │  
└────────────────────────────────┴─────────────────┴────────────────────────────────────────────────────────────────────────────────────┴────────────┘  
```  
  
下面创建一个表, 写入大量数据, 让数据大于内存限制  
  
```  
create table test (id int8, info text, crt_time timestamp, c1 int8);  
  
insert into test select generate_series, md5(random()::text), now()+(generate_series||' second')::interval, random()*10000 from generate_series(1,100000000);  
insert into test select * from test;  
insert into test select generate_series, md5(random()::text), now()+(generate_series||' second')::interval, random()*10000 from generate_series(1,100000000);  
insert into test select generate_series, md5(random()::text), now()+(generate_series||' second')::interval, random()*10000 from generate_series(1,100000000);  
```  
  
查询数据库大小  
  
```  
D PRAGMA database_size;  
┌───────────────┬────────────┬──────────────┬─────────────┬─────────────┬──────────┬──────────────┬──────────────┐  
│ database_size │ block_size │ total_blocks │ used_blocks │ free_blocks │ wal_size │ memory_usage │ memory_limit │  
├───────────────┼────────────┼──────────────┼─────────────┼─────────────┼──────────┼──────────────┼──────────────┤  
│ 18.4GB        │ 262144     │ 70486        │ 70441       │ 45          │ 0 bytes  │ 966.2MB      │ 1.0GB        │  
└───────────────┴────────────┴──────────────┴─────────────┴─────────────┴──────────┴──────────────┴──────────────┘  
Run Time (s): real 0.001 user 0.000734 sys 0.000111  
```  
  
将内存限制缩小到`1GB`  
  
```  
D PRAGMA memory_limit='1GB';  
Run Time (s): real 1.621 user 0.080263 sys 1.502544  
```  
  
再次查询, 由于ID非常多, 需要很大的hash table, 超出内存限制, 报错  
  
```  
D select count(distinct id) from test;  
Run Time (s): real 1.095 user 5.476447 sys 1.655423  
Error: Out of Memory Error: could not allocate block of 262144 bytes (999817728/1000000000 used)   
```  
  
其他未超出内存限制的sql可以正常执行  
  
```  
D select count(*) from test;  
┌──────────────┐  
│ count_star() │  
├──────────────┤  
│ 400000000    │  
└──────────────┘  
Run Time (s): real 69.039 user 38.860264 sys 10.419579  
  
D select count(distinct c1) from test;  
┌────────────────────┐  
│ count(DISTINCT c1) │  
├────────────────────┤  
│ 10001              │  
└────────────────────┘  
Run Time (s): real 3.964 user 28.897850 sys 0.471984  
```  
  
执行检查点  
  
```  
D checkpoint;  
Run Time (s): real 0.001 user 0.000195 sys 0.000021  
```  
  
进到数据目录, 查询数据文件如下  
  
```  
drwxr-xr-x   5 digoal  staff   160B Sep  1 14:54 digoal.db.tmp  
-rw-r--r--   1 digoal  staff    17G Sep  1 14:57 digoal.db  
-rw-r--r--   1 digoal  staff     0B Sep  1 14:57 digoal.db.wal  
  
  
IT-C02YW2EFLVDL:release digoal$ cd digoal.db.tmp  
IT-C02YW2EFLVDL:digoal.db.tmp digoal$ ll  
total 3072  
drwxr-xr-x  21 digoal  staff   672B Sep  1 14:31 ..  
drwxr-xr-x   5 digoal  staff   160B Sep  1 14:54 .  
-rw-r--r--   1 digoal  staff   512K Sep  1 14:57 duckdb_temp_storage-2.tmp  
-rw-r--r--   1 digoal  staff   512K Sep  1 14:57 duckdb_temp_storage-0.tmp  
-rw-r--r--   1 digoal  staff   512K Sep  1 14:57 duckdb_temp_storage-1.tmp  
```  
  
  
## 推荐的使用姿势  
  
将数据存放在 parquet 文件中, 由于parquet支持pushdown filter\projection, 支持分区, 支持并行.  查询速度是非常快的.   甚至可以使用远程parquet文件存储 (s3, https, oss) .   
  
例子:    
  
将数据导出到parquet文件  
  
```  
COPY test TO 'test.parquet' (FORMAT 'parquet');   
  
or 导出整个数据库  
  
EXPORT DATABASE '/Users/digoal/digoal_parquet' (FORMAT PARQUET);  
```  
  
然后删除表, 以视图替代:   
  
```  
drop table test;  
  
CREATE VIEW test AS SELECT * FROM read_parquet('test.parquet');  
```  
  
查询试图:   
  
```  
D describe test;  
┌─────────────┬─────────────┬──────┬─────┬─────────┬───────┐  
│ column_name │ column_type │ null │ key │ default │ extra │  
├─────────────┼─────────────┼──────┼─────┼─────────┼───────┤  
│ id          │ BIGINT      │ YES  │     │         │       │  
│ info        │ VARCHAR     │ YES  │     │         │       │  
│ crt_time    │ TIMESTAMP   │ YES  │     │         │       │  
│ c1          │ BIGINT      │ YES  │     │         │       │  
└─────────────┴─────────────┴──────┴─────┴─────────┴───────┘  
```  
  
  
```  
D select count(*) from test;  
┌──────────────┐  
│ count_star() │  
├──────────────┤  
│ 400000000    │  
└──────────────┘  
Run Time (s): real 0.107 user 0.600973 sys 0.013549  
  
D explain select count(*) from test;  
  
┌─────────────────────────────┐  
│┌───────────────────────────┐│  
││       Physical Plan       ││  
│└───────────────────────────┘│  
└─────────────────────────────┘  
┌───────────────────────────┐  
│    UNGROUPED_AGGREGATE    │  
│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │  
│        count_star()       │  
└─────────────┬─────────────┘                               
┌─────────────┴─────────────┐  
│         PROJECTION        │  
│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │  
│             42            │  
└─────────────┬─────────────┘                               
┌─────────────┴─────────────┐  
│        PARQUET_SCAN       │  
│   ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─   │  
│        EC=400000000       │  
└───────────────────────────┘                      
  
D select count(distinct c1) from test;  
┌────────────────────┐  
│ count(DISTINCT c1) │  
├────────────────────┤  
│ 10001              │  
└────────────────────┘  
Run Time (s): real 4.626 user 32.262045 sys 1.084296  
```  
  
```  
D select count(distinct id) from test;  
Run Time (s): real 1.044 user 5.788092 sys 1.084814  
Error: Out of Memory Error: could not allocate block of 262144 bytes (999924527/1000000000 used)   
```  
  
```  
checkpoint;  
  
.quit  
```  
  
重新启动, 视图依旧存在 , 但是需要注意duckdb的数据文件没有收缩.  所以直接弃用digoal.db, 重新启动一个数据库.  
  
```  
-rw-r--r--   1 digoal  staff    17G Sep  1 15:30 digoal.db  
  
rm -rf digoal.db*  
```  
  
  
```  
$ ./duckdb ./digoal.db.new  
v0.4.1-dev2371 3825e0ee7  
Enter ".help" for usage hints.  
  
D CREATE VIEW test AS SELECT * FROM read_parquet('test.parquet');  
  
D .timer on  
  
D select count(distinct c1) from test;  
┌────────────────────┐  
│ count(DISTINCT c1) │  
├────────────────────┤  
│ 10001              │  
└────────────────────┘  
Run Time (s): real 4.525 user 32.616092 sys 0.790214  
  
D select count(distinct id) from test;  
┌────────────────────┐  
│ count(DISTINCT id) │  
├────────────────────┤  
│ 100000000          │  
└────────────────────┘  
Run Time (s): real 36.541 user 151.904681 sys 37.632788  
```  
  
  
  
  
  
## 参考  
https://duckdb.org/docs/sql/pragmas  
  
https://duckdb.org/docs/sql/configuration  
  
https://duckdb.org/docs/api/cli  
  
[《DuckDB 读写 Parquet 文件 - 同时支持远程s3, oss, http等parquet文件读写》](../202209/20220901_01.md)    
    
  
#### [期望 PostgreSQL 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
  
  
#### [PolarDB for PostgreSQL云原生分布式开源数据库](https://github.com/ApsaraDB/PolarDB-for-PostgreSQL "57258f76c37864c6e6d23383d05714ea")
  
  
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
  
  
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
  
  
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
  
  
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
  
  
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
  
  
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")
  
